{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZMxUtyKnG3C"
      },
      "source": [
        "# üó£Ô∏è Ferramenta de transcri√ß√£o de √°udio e an√°lise de conte√∫do em pesquisas qualitativas\n",
        "\n",
        "### Gabriel de Antonio Mazetto  \n",
        "**Projeto de Inicia√ß√£o Cient√≠fica - PIBIC/CNPq/INPE**  \n",
        "- Orientadora: Dra. Minella Alves Martins (INPE)\n",
        "- Co-orientadora: Dra. Maria Paula Pires de Oliveira (PUC-Campinas)  \n",
        "- Colaboradora: Dra Denise Helena Lombardo Ferreira (PUC-Campinas)\n",
        "- Per√≠odo: Agosto/2024 ‚Äî Agosto/2025  \n",
        "\n",
        "**Apoio Institucional:**   \n",
        "- Ferramenta elaborado com apoio do Conselho Nacional de Desenvolvimento Cient√≠fico e Tecnol√≥gico (bolsa PIBIC/CNPq/INPE) e da Coordena√ß√£o de Aperfei√ßoamento de Pessoal de N√≠vel Superior - Brasil (CAPES) ‚Äì C√≥digo de Financiamento 001.\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Objetivo Geral\n",
        "\n",
        "Este notebook foi desenvolvido como ferramenta de apoio √† an√°lise de conte√∫do em pesquisas qualitativas. Ele automatiza etapas como transcri√ß√£o, explora√ß√£o sem√¢ntica e compara√ß√£o de m√∫ltiplas entrevistas, depoimentos e outros registros textuais.\n",
        "\n",
        "---\n",
        "\n",
        "## üõ†Ô∏è Funcionalidades\n",
        "\n",
        "- **Transcri√ß√£o autom√°tica** de arquivos de √°udio e v√≠deo utilizando o modelo **Whisper (OpenAI)**;\n",
        "- **Nuvens de palavras e gr√°ficos de frequ√™ncia**, para explorar os termos mais recorrentes;\n",
        "- **Grafo de coocorr√™ncia de palavras**, visualizando rela√ß√µes entre termos em uma rede sem√¢ntica;\n",
        "- **Reconhecimento de Entidades Nomeadas (NER)**: identifica√ß√£o de pessoas, locais e organiza√ß√µes mencionados nos discursos;\n",
        "- **Codifica√ß√£o.**\n",
        "- **Buscas interativas** no conte√∫do transcrito, com diferentes abordagens:\n",
        "  - **Busca exata (booleana)** por termos espec√≠ficos;\n",
        "  - **Busca por lema**, que considera diferentes formas de uma mesma palavra (ex: \"comer\", \"comeu\", \"comendo\");\n",
        "  - **Busca por similaridade sem√¢ntica de frases** e **similaridade de palavras**, utilizando embeddings gerados com o modelo `distiluse-base-multilingual-cased-v1` (via *SentenceTransformers*) ‚Äî cada palavra √© transformada em vetor, permitindo buscas por contexto e significado;\n",
        "  - **Busca por palavras coocorrentes a um termo-alvo**, que complementa a an√°lise do grafo de coocorr√™ncia ao destacar as palavras que frequentemente aparecem no mesmo contexto da palavra buscada.\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "SJl7HJOeo0-P"
      },
      "outputs": [],
      "source": [
        "# @title ‚öôÔ∏è 1. Instalando depend√™ncias e reiniciando o ambiente\n",
        "# @markdown > Execute esta c√©lula e aguarde at√© que a mensagem \"‚úÖ Instala√ß√µes conclu√≠das.\" e \"‚ÄºÔ∏è REINICIANDO O AMBIENTE...\" apare√ßa.\n",
        "# @markdown >\n",
        "# @markdown > Ap√≥s o rein√≠cio autom√°tico, voc√™ poder√° seguir para a pr√≥xima etapa.\n",
        "\n",
        "!pip install numpy==2.0.0 scipy==1.14.0 -q\n",
        "\n",
        "!pip install git+https://github.com/openai/whisper.git -q\n",
        "!pip install ffmpeg-python -q\n",
        "!pip install pydrive2 -q\n",
        "\n",
        "!pip install spacy -q\n",
        "!pip install wordcloud -q\n",
        "\n",
        "!pip install gensim -q\n",
        "!pip install sentence-transformers -q\n",
        "!pip install ipywidgets -q\n",
        "\n",
        "!python -m spacy download pt_core_news_lg -q\n",
        "\n",
        "from IPython.display import clear_output\n",
        "clear_output()\n",
        "\n",
        "print(\"‚úÖ Instala√ß√µes conclu√≠das.\")\n",
        "print(\"‚ÄºÔ∏è REINICIANDO O AMBIENTE AUTOMATICAMENTE para carregar as novas bibliotecas...\")\n",
        "\n",
        "import time\n",
        "time.sleep(1)\n",
        "\n",
        "import os\n",
        "\n",
        "os.kill(os.getpid(), 9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "A3Hj_ZBBnCIs"
      },
      "outputs": [],
      "source": [
        "# @title üìÇ 2. Especifique a Origem e o(s) Arquivo(s)\n",
        "from pydrive2.auth import GoogleAuth\n",
        "from pydrive2.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import os\n",
        "import re\n",
        "\n",
        "# --- INSTRU√á√ïES PARA O USU√ÅRIO ---\n",
        "# @markdown ### üîπ 1. Selecione a Origem dos Arquivos\n",
        "# @markdown Marque a caixa para buscar os arquivos no seu Google Drive. Se deix√°-la desmarcada, o sistema procurar√° por arquivos que voc√™ subiu diretamente para o ambiente do Colab.\n",
        "usar_google_drive = False #@param {type:\"boolean\"}\n",
        "\n",
        "# @markdown ---\n",
        "# @markdown ### üîπ 2. Defina os Arquivos\n",
        "# @markdown **Se estiver usando o Google Drive:** Preencha o `nome_da_pasta`.\n",
        "# @markdown - Para um ou mais **arquivos espec√≠ficos**, preencha o `nome_do_arquivo` (para m√∫ltiplos arquivos, separe os nomes por v√≠rgula).\n",
        "# @markdown - Para **TODOS os arquivos** da pasta, marque a caixa `processar_todos_os_arquivos`.\n",
        "# @markdown ---\n",
        "# @markdown **Se estiver usando Arquivos Locais:** Deixe o `nome_da_pasta` em branco.\n",
        "# @markdown - Para um ou mais **arquivos espec√≠ficos**, preencha o `nome_do_arquivo` (para m√∫ltiplos arquivos, separe os nomes por v√≠rgula).\n",
        "# @markdown - Para **TODOS os arquivos** de m√≠dia no ambiente, marque a caixa `processar_todos_os_arquivos`.\n",
        "\n",
        "nome_da_pasta = \"\"  # @param {type:\"string\"}\n",
        "nome_do_arquivo = \"\"  # @param {type:\"string\"}\n",
        "processar_todos_os_arquivos = True #@param {type:\"boolean\"}\n",
        "# -----------------------------------\n",
        "\n",
        "file_paths = []\n",
        "folder_path_stripped = nome_da_pasta.strip()\n",
        "file_names_stripped = nome_do_arquivo.strip()\n",
        "\n",
        "if usar_google_drive:\n",
        "    # --- M√âTODO 1: BUSCAR NO GOOGLE DRIVE ---\n",
        "    print(\"‚ñ∂Ô∏è Modo Google Drive selecionado.\")\n",
        "    if not folder_path_stripped:\n",
        "        print(\"‚ùå ERRO: O modo Google Drive est√° selecionado, mas o nome da pasta n√£o foi preenchido.\")\n",
        "    else:\n",
        "        try:\n",
        "            auth.authenticate_user()\n",
        "            gauth = GoogleAuth()\n",
        "            gauth.credentials = GoogleCredentials.get_application_default()\n",
        "            drive = GoogleDrive(gauth)\n",
        "\n",
        "            # Navega pela estrutura de pastas\n",
        "            path_parts = [part.strip() for part in folder_path_stripped.split('/') if part.strip()]\n",
        "            parent_id = 'root'\n",
        "            target_folder_id = None\n",
        "            for i, part in enumerate(path_parts):\n",
        "                query = f\"'{parent_id}' in parents and title='{part}' and mimeType='application/vnd.google-apps.folder' and trashed=false\"\n",
        "                folder_list = drive.ListFile({'q': query}).GetList()\n",
        "                if not folder_list:\n",
        "                    print(f\"‚ùå ERRO: A subpasta '{part}' n√£o foi encontrada.\")\n",
        "                    target_folder_id = None\n",
        "                    break\n",
        "                parent_id = folder_list[0]['id']\n",
        "                target_folder_id = parent_id\n",
        "\n",
        "            if target_folder_id:\n",
        "                print(f\"‚úÖ Pasta final '{folder_path_stripped}' encontrada.\")\n",
        "                file_list = []\n",
        "\n",
        "                if processar_todos_os_arquivos:\n",
        "                    print(\"Buscando todos os arquivos de m√≠dia na pasta...\")\n",
        "                    query = f\"'{target_folder_id}' in parents and (mimeType contains 'video/' or mimeType contains 'audio/') and trashed=false\"\n",
        "                    file_list = drive.ListFile({'q': query}).GetList()\n",
        "                    if not file_list: print(\"‚ùå Nenhum arquivo de √°udio ou v√≠deo encontrado na pasta.\")\n",
        "\n",
        "                else:\n",
        "                    specific_filenames = [name.strip() for name in file_names_stripped.split(',') if name.strip()]\n",
        "                    if not specific_filenames:\n",
        "                         print(\"‚ùå ERRO: Nenhum nome de arquivo foi preenchido.\")\n",
        "                    else:\n",
        "                        title_queries = []\n",
        "                        for name in specific_filenames:\n",
        "                            escaped_name = name.replace(\"'\", \"\\\\'\")\n",
        "                            title_queries.append(f\"title='{escaped_name}'\")\n",
        "                        title_query_part = \" or \".join(title_queries)\n",
        "                        full_query = f\"'{target_folder_id}' in parents and ({title_query_part}) and trashed=false\"\n",
        "                        print(f\"Buscando pelos arquivos espec√≠ficos: {specific_filenames}...\")\n",
        "                        file_list = drive.ListFile({'q': full_query}).GetList()\n",
        "\n",
        "                # Faz o download dos arquivos encontrados\n",
        "                if not file_list:\n",
        "                     if not processar_todos_os_arquivos: print(f\"‚ùå Nenhum dos arquivos especificados foi encontrado na pasta.\")\n",
        "                else:\n",
        "                    print(f\"Encontrado(s) {len(file_list)} arquivo(s). Iniciando download...\")\n",
        "                    for f in file_list:\n",
        "                        print(f\" -> Baixando '{f['title']}'...\")\n",
        "                        colab_file_path = os.path.join(\"/content/\", f['title'])\n",
        "                        f.GetContentFile(colab_file_path)\n",
        "                        file_paths.append(colab_file_path)\n",
        "                    print(\"‚úÖ Todos os downloads foram conclu√≠dos!\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Ocorreu um erro durante a conex√£o com o Google Drive: {e}\")\n",
        "\n",
        "else:\n",
        "    # --- M√âTODO 2: USAR ARQUIVOS LOCAIS DO COLAB ---\n",
        "    print(\"‚ñ∂Ô∏è Modo Local selecionado.\")\n",
        "    if processar_todos_os_arquivos:\n",
        "        print(\"Buscando todos os arquivos de m√≠dia no ambiente local...\")\n",
        "        valid_extensions = ['.mp4', '.m4a', '.mp3', '.mov', '.wav', '.flac']\n",
        "        all_local_files = [f for f in os.listdir('/content/') if os.path.isfile(os.path.join('/content/', f))]\n",
        "        media_files = [f for f in all_local_files if any(f.lower().endswith(ext) for ext in valid_extensions)]\n",
        "\n",
        "        if not media_files:\n",
        "            print(\"‚ùå Nenhum arquivo de m√≠dia compat√≠vel encontrado no ambiente do Colab.\")\n",
        "        else:\n",
        "            for fname in media_files:\n",
        "                file_paths.append(os.path.join(\"/content/\", fname))\n",
        "            print(f\"‚úÖ Arquivos locais de m√≠dia encontrados: {', '.join(media_files)}\")\n",
        "    else:\n",
        "        # L√≥gica para nomes espec√≠ficos\n",
        "        local_filenames = [name.strip() for name in file_names_stripped.split(',') if name.strip()]\n",
        "        if not local_filenames:\n",
        "            print(\"‚ùå ERRO: Nenhum nome de arquivo foi preenchido.\")\n",
        "        else:\n",
        "            print(f\"Verificando a exist√™ncia de {len(local_filenames)} arquivo(s) locais...\")\n",
        "            for fname in local_filenames:\n",
        "                local_path = os.path.join(\"/content/\", fname)\n",
        "                if os.path.exists(local_path):\n",
        "                    file_paths.append(local_path)\n",
        "                    print(f\"  -> ‚úÖ Arquivo '{fname}' encontrado.\")\n",
        "                else:\n",
        "                    print(f\"  -> ‚ùå ERRO: O arquivo '{fname}' n√£o foi encontrado no ambiente do Colab.\")\n",
        "\n",
        "# --- Verifica√ß√£o final ---\n",
        "if file_paths:\n",
        "    print(f\"\\n‚û°Ô∏è {len(file_paths)} arquivo(s) pronto(s) para serem processados na pr√≥xima c√©lula.\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è Nenhum arquivo foi definido para processamento. Verifique os erros acima.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "qZa8K_1TnbVD"
      },
      "outputs": [],
      "source": [
        "# @title üó£Ô∏è 3A. Transcrever Arquivo(s) de √Åudio/V√≠deo\n",
        "# @markdown > Esta c√©lula ir√° transcrever todos os arquivos definidos na C√©lula 2.\n",
        "\n",
        "# @markdown > As transcri√ß√µes a seguir foram geradas com o aux√≠lio da ferramenta de intelig√™ncia artificial Whisper. Embora a tecnologia seja avan√ßada, erros de transcri√ß√£o podem ocorrer. Recomenda-se a verifica√ß√£o com o √°udio original para garantir a m√°xima precis√£o das informa√ß√µes.\n",
        "\n",
        "# @markdown ---\n",
        "# @markdown **Modo de An√°lise para M√∫ltiplos Arquivos:**\n",
        "# @markdown Marque para juntar as transcri√ß√µes em uma an√°lise conjunta. Desmarque para analisar cada uma individualmente.\n",
        "juntar_arquivos_para_analise_conjunta = False # @param {type:\"boolean\"}\n",
        "\n",
        "import whisper\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "\n",
        "def format_timestamp(seconds):\n",
        "    try:\n",
        "        milliseconds = round((seconds - int(seconds)) * 1000)\n",
        "        td = pd.to_timedelta(seconds, unit='s')\n",
        "        h, m, s = td.components.hours + (td.components.days * 24), td.components.minutes, td.components.seconds\n",
        "        return f\"{h:02d}:{m:02d}:{s:02d}.{milliseconds:03d}\"\n",
        "    except: return \"00:00:00.000\"\n",
        "\n",
        "if 'file_paths' not in globals() or not file_paths:\n",
        "    print(\"‚ùå ERRO: Nenhum arquivo para transcrever. Por favor, execute a C√©lula 2 primeiro.\")\n",
        "else:\n",
        "    print(\"Carregando o modelo de transcri√ß√£o (whisper medium)...\")\n",
        "    model = whisper.load_model(\"medium\")\n",
        "\n",
        "    if 'analises_individuais' in globals(): del analises_individuais\n",
        "    if 'df_transcription' in globals(): del df_transcription\n",
        "\n",
        "    all_transcription_data = []\n",
        "    analises_individuais = {}\n",
        "    time_offset = 0.0\n",
        "\n",
        "    is_joint_analysis = juntar_arquivos_para_analise_conjunta or len(file_paths) == 1\n",
        "\n",
        "    print(f\"\\nIniciando a transcri√ß√£o de {len(file_paths)} arquivo(s)...\")\n",
        "    for path in file_paths:\n",
        "        filename = os.path.basename(path)\n",
        "        print(\"\\n\" + \"=\"*50 + f\"\\nTranscrevendo: {filename}\\n\" + \"=\"*50)\n",
        "\n",
        "        result = model.transcribe(path, verbose=True, language='pt', fp16=False)\n",
        "\n",
        "        current_file_data = []\n",
        "        text_with_timestamps = \"\"\n",
        "        for segment in result['segments']:\n",
        "            start_f = format_timestamp(segment['start'])\n",
        "            end_f = format_timestamp(segment['end'])\n",
        "            text = segment['text'].strip()\n",
        "            text_with_timestamps += f\"[{start_f} --> {end_f}] {text}\\n\"\n",
        "            current_file_data.append({\"start\": segment['start'], \"end\": segment['end'], \"text\": text})\n",
        "\n",
        "        base_name, _ = os.path.splitext(filename)\n",
        "        output_filename_ts = f\"transcricao_com-tempo_{base_name}.txt\"\n",
        "        with open(output_filename_ts, 'w', encoding='utf-8') as f:\n",
        "            f.write(text_with_timestamps)\n",
        "        print(f\"\\n‚úÖ Transcri√ß√£o individual salva em '{output_filename_ts}'\")\n",
        "\n",
        "        output_filename_text = f\"transcricao_texto-puro_{base_name}.txt\"\n",
        "        with open(output_filename_text, 'w', encoding='utf-8') as f:\n",
        "            f.write(\" \".join([item['text'] for item in current_file_data]))\n",
        "        print(f\"‚úÖ Arquivo de texto puro '{output_filename_text}' salvo no ambiente.\")\n",
        "\n",
        "        print(\"Iniciando download do arquivo com marca√ß√£o de tempo...\")\n",
        "        files.download(output_filename_ts)\n",
        "        time.sleep(1)\n",
        "\n",
        "        if is_joint_analysis:\n",
        "            for item in current_file_data:\n",
        "                item['start'] += time_offset\n",
        "                item['end'] += time_offset\n",
        "            all_transcription_data.extend(current_file_data)\n",
        "            if all_transcription_data:\n",
        "                time_offset = all_transcription_data[-1]['end']\n",
        "        else:\n",
        "            df_individual = pd.DataFrame(current_file_data)\n",
        "            if not df_individual.empty:\n",
        "                analises_individuais[filename] = {'df': df_individual, 'full_text': \" \".join(df_individual['text'])}\n",
        "\n",
        "    if is_joint_analysis:\n",
        "        df_transcription = pd.DataFrame(all_transcription_data)\n",
        "        full_text = \" \".join(df_transcription['text'])\n",
        "        juntar_arquivos_para_analise_conjunta = True\n",
        "        if not df_transcription.empty:\n",
        "            print(\"\\n‚úÖ Processamento conclu√≠do! Modo de An√°lise Conjunta ativado para as pr√≥ximas c√©lulas.\")\n",
        "    else:\n",
        "        juntar_arquivos_para_analise_conjunta = False\n",
        "        if analises_individuais:\n",
        "             print(f\"\\n‚úÖ Processamento conclu√≠do! {len(analises_individuais)} transcri√ß√µes foram carregadas para an√°lise individual.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ch8Z2pjBXEvK"
      },
      "outputs": [],
      "source": [
        "# @title üìù 3B. Carregar Transcri√ß√£o(√µes) de Arquivo(s) .txt\n",
        "# @markdown > Se voc√™ j√° transcreveu seus √°udios em uma sess√£o anterior e baixou os arquivos `.txt`, use esta c√©lula para carreg√°-los diretamente e pular a etapa de transcri√ß√£o ao vivo.\n",
        "\n",
        "# @markdown > **Instru√ß√£o:** Suba os arquivos `.txt` para o ambiente do Colab (no painel 'Arquivos' √† esquerda) e depois preencha as op√ß√µes abaixo.\n",
        "\n",
        "# @markdown > **Formatos Aceitos:**\n",
        "# @markdown > * **Com Timestamp:** Arquivos padr√£o de transcri√ß√£o, com o tempo de cada fala. Ex: `[00:00:01.234 --> 00:00:05.678] Texto da transcri√ß√£o.`\n",
        "# @markdown > * **Texto Simples:** Arquivos `.txt` contendo apenas o texto corrido. O conte√∫do ser√° automaticamente dividido em frases com base na pontua√ß√£o (ponto, v√≠rgula, etc.) e quebras de linha.\n",
        "# @markdown ---\n",
        "# @markdown ---\n",
        "# @markdown **Op√ß√£o 1: Listar Nomes dos Arquivos**\n",
        "\n",
        "# @markdown Coloque os nomes dos arquivos .txt (separados por v√≠rgula).\n",
        "# @markdown *Exemplo: `transcricao_parte1.txt, transcricao_parte2.txt`*\n",
        "transcript_filenames_str = \"\" # @param {type:\"string\"}\n",
        "\n",
        "# @markdown ---\n",
        "# @markdown **Op√ß√£o 2: Processar Todos os Arquivos .txt**\n",
        "\n",
        "# @markdown Se marcado, o campo acima ser√° ignorado e o sistema buscar√° todos os arquivos `.txt` no ambiente.\n",
        "processar_todos_os_arquivos_txt = True # @param {type:\"boolean\"}\n",
        "\n",
        "# @markdown ---\n",
        "# @markdown **Modo de An√°lise para M√∫ltiplos Arquivos:**\n",
        "# @markdown Marque para juntar os arquivos em uma an√°lise conjunta. Desmarque para analisar cada um individualmente.\n",
        "juntar_arquivos_para_analise_conjunta = False # @param {type:\"boolean\"}\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "import os\n",
        "\n",
        "# --- Fun√ß√£o para converter timestamp (HH:MM:SS.ms) para segundos ---\n",
        "def timestamp_to_seconds(ts_str):\n",
        "    try:\n",
        "        h, m, s_ms = ts_str.split(':')\n",
        "        s, ms = s_ms.split('.')\n",
        "        return int(h) * 3600 + int(m) * 60 + int(s) + int(ms) / 1000.0\n",
        "    except ValueError: return 0.0\n",
        "# ------------------------------------------------------------------\n",
        "\n",
        "file_list = []\n",
        "if processar_todos_os_arquivos_txt:\n",
        "    print(\"‚ñ∂Ô∏è Modo 'Processar Todos' selecionado. Buscando por arquivos .txt...\")\n",
        "    file_list = sorted([f for f in os.listdir('.') if f.endswith('.txt')])\n",
        "    if not file_list:\n",
        "        print(\"‚ùå Nenhum arquivo .txt foi encontrado no ambiente do Colab.\")\n",
        "    else:\n",
        "        print(f\"‚úÖ Arquivos .txt encontrados: {', '.join(file_list)}\")\n",
        "else:\n",
        "    print(\"‚ñ∂Ô∏è Modo 'Nomes Espec√≠ficos' selecionado.\")\n",
        "    file_list = [name.strip() for name in transcript_filenames_str.split(',') if name.strip()]\n",
        "    if not file_list:\n",
        "        print(\"‚ùå ERRO: Nenhum nome de arquivo foi inserido no campo de texto.\")\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "\n",
        "if file_list:\n",
        "    if 'analises_individuais' in globals(): del analises_individuais\n",
        "    if 'df_transcription' in globals(): del df_transcription\n",
        "\n",
        "    if juntar_arquivos_para_analise_conjunta and len(file_list) > 1:\n",
        "        print(\"\\n‚ñ∂Ô∏è Modo de An√°lise Conjunta selecionado.\")\n",
        "        all_transcription_data = []\n",
        "        time_offset = 0.0\n",
        "        processed_files, failed_files = [], []\n",
        "        for filename in file_list:\n",
        "            if not os.path.exists(filename):\n",
        "                print(f\"‚ö†Ô∏è ATEN√á√ÉO: Arquivo '{filename}' n√£o encontrado! Pulando...\")\n",
        "                failed_files.append(filename)\n",
        "                continue\n",
        "            print(f\"Processando e juntando '{filename}'...\")\n",
        "            processed_files.append(filename)\n",
        "            with open(filename, 'r', encoding='utf-8') as f:\n",
        "                content = f.read()\n",
        "                f.seek(0)\n",
        "\n",
        "                has_timestamps = re.search(r'\\[(\\d{2}:\\d{2}:\\d{2}\\.\\d{3})\\s-->\\s(\\d{2}:\\d{2}:\\d{2}\\.\\d{3})\\]', content)\n",
        "\n",
        "                if has_timestamps:\n",
        "                    for line in f:\n",
        "                        match = re.match(r'\\[(\\d{2}:\\d{2}:\\d{2}\\.\\d{3})\\s-->\\s(\\d{2}:\\d{2}:\\d{2}\\.\\d{3})\\]\\s*(.*)', line)\n",
        "                        if match:\n",
        "                            start_str, end_str, text = match.groups()\n",
        "                            all_transcription_data.append({\n",
        "                                \"start\": timestamp_to_seconds(start_str) + time_offset,\n",
        "                                \"end\": timestamp_to_seconds(end_str) + time_offset,\n",
        "                                \"text\": text.strip()\n",
        "                            })\n",
        "                else:\n",
        "                    print(f\"   ‚ÑπÔ∏è INFO: Arquivo '{filename}' n√£o cont√©m timestamps. O texto ser√° dividido por frases.\")\n",
        "                    sentences = [s.strip() for s in re.split(r'[,.!?\\n]+', content) if s.strip()]\n",
        "                    for text in sentences:\n",
        "                        all_transcription_data.append({\n",
        "                            \"start\": 0.0 + time_offset,\n",
        "                            \"end\": 0.0 + time_offset,\n",
        "                            \"text\": text\n",
        "                        })\n",
        "\n",
        "            if all_transcription_data and has_timestamps:\n",
        "                if all_transcription_data[-1]['end'] > time_offset:\n",
        "                     time_offset = all_transcription_data[-1]['end']\n",
        "\n",
        "        df_transcription = pd.DataFrame(all_transcription_data)\n",
        "        if not df_transcription.empty:\n",
        "            print(\"\\n‚úÖ Processamento conclu√≠do! Todos os arquivos foram unidos.\")\n",
        "        else:\n",
        "            print(\"\\n‚ö†Ô∏è Nenhum dado de transcri√ß√£o foi carregado.\")\n",
        "\n",
        "    else:\n",
        "        if len(file_list) > 1:\n",
        "            print(\"\\n‚ñ∂Ô∏è Modo de An√°lise Individual selecionado.\")\n",
        "        analises_individuais = {}\n",
        "        processed_files, failed_files = [], []\n",
        "        for filename in file_list:\n",
        "            if not os.path.exists(filename):\n",
        "                print(f\"‚ö†Ô∏è ATEN√á√ÉO: Arquivo '{filename}' n√£o encontrado! Pulando...\")\n",
        "                failed_files.append(filename)\n",
        "                continue\n",
        "            print(f\"Processando '{filename}'...\")\n",
        "            processed_files.append(filename)\n",
        "            transcription_data = []\n",
        "            with open(filename, 'r', encoding='utf-8') as f:\n",
        "                content = f.read()\n",
        "                f.seek(0)\n",
        "\n",
        "                has_timestamps = re.search(r'\\[(\\d{2}:\\d{2}:\\d{2}\\.\\d{3})\\s-->\\s(\\d{2}:\\d{2}:\\d{2}\\.\\d{3})\\]', content)\n",
        "\n",
        "                if has_timestamps:\n",
        "                     for line in f:\n",
        "                        match = re.match(r'\\[(\\d{2}:\\d{2}:\\d{2}\\.\\d{3})\\s-->\\s(\\d{2}:\\d{2}:\\d{2}\\.\\d{3})\\]\\s*(.*)', line)\n",
        "                        if match:\n",
        "                            start_str, end_str, text = match.groups()\n",
        "                            transcription_data.append({\n",
        "                                \"start\": timestamp_to_seconds(start_str),\n",
        "                                \"end\": timestamp_to_seconds(end_str),\n",
        "                                \"text\": text.strip()\n",
        "                            })\n",
        "                else:\n",
        "                    print(f\"   ‚ÑπÔ∏è INFO: Arquivo '{filename}' n√£o cont√©m timestamps. O texto ser√° dividido por frases.\")\n",
        "                    sentences = [s.strip() for s in re.split(r'[,.!?\\n]+', content) if s.strip()]\n",
        "                    for text in sentences:\n",
        "                         transcription_data.append({\n",
        "                            \"start\": 0.0,\n",
        "                            \"end\": 0.0,\n",
        "                            \"text\": text\n",
        "                        })\n",
        "\n",
        "            df_individual = pd.DataFrame(transcription_data)\n",
        "            if not df_individual.empty:\n",
        "                analises_individuais[filename] = {'df': df_individual, 'full_text': \" \".join(df_individual['text'])}\n",
        "\n",
        "        if analises_individuais:\n",
        "            print(f\"\\n‚úÖ Processamento conclu√≠do! {len(analises_individuais)} arquivo(s) foram carregados para an√°lise individual.\")\n",
        "        else:\n",
        "            print(\"\\n‚ö†Ô∏è Nenhum arquivo v√°lido foi processado.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ypBh2B4hz7_E"
      },
      "outputs": [],
      "source": [
        "# @title üõ†Ô∏è 4. Prepara√ß√£o do Texto para An√°lise\n",
        "# @markdown > **Execute esta c√©lula antes de prosseguir.**\n",
        "# @markdown >\n",
        "# @markdown > Ela realiza o processamento lingu√≠stico dos textos, criando as bases de dados necess√°rias para todas as an√°lises das c√©lulas seguintes.\n",
        "\n",
        "import spacy\n",
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Carrega o modelo de linguagem uma √∫nica vez\n",
        "try:\n",
        "    if 'nlp' not in globals():\n",
        "        print(\"Carregando modelo de linguagem (spaCy)...\")\n",
        "        nlp = spacy.load(\"pt_core_news_lg\")\n",
        "        print(\"‚úÖ Modelo carregado.\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Ocorreu um erro ao carregar o modelo spaCy: {e}\")\n",
        "\n",
        "# Verifica se a an√°lise conjunta foi selecionada e prepara os dados\n",
        "if 'juntar_arquivos_para_analise_conjunta' in globals() and juntar_arquivos_para_analise_conjunta:\n",
        "    if 'df_transcription' in globals() and not df_transcription.empty:\n",
        "        print(\"\\n--- PREPARANDO DADOS PARA AN√ÅLISE CONJUNTA ---\")\n",
        "        # Garante que as vari√°veis globais sejam criadas ou atualizadas\n",
        "        full_text = \" \".join(df_transcription['text'])\n",
        "        print(\"Processando texto combinado (pode demorar em textos longos)...\")\n",
        "        doc = nlp(full_text)\n",
        "        lemmatized_sents = [\" \".join([token.lemma_.lower() for token in nlp(sent)]) for sent in df_transcription['text']]\n",
        "        df_transcription['lemmatized_text'] = lemmatized_sents\n",
        "        print(\"‚úÖ Dados para an√°lise conjunta est√£o prontos.\")\n",
        "    else:\n",
        "        print(\"‚ùå Nenhum dado para an√°lise conjunta encontrado. Execute a C√©lula 3A ou 3B primeiro.\")\n",
        "\n",
        "# Verifica se a an√°lise individual foi selecionada e prepara os dados\n",
        "elif 'analises_individuais' in globals() and analises_individuais:\n",
        "    print(\"\\n--- PREPARANDO DADOS PARA AN√ÅLISE INDIVIDUAL ---\")\n",
        "    for filename, data in analises_individuais.items():\n",
        "        print(f\"Processando '{filename}'...\")\n",
        "        # Garante que as vari√°veis necess√°rias existam dentro de cada dicion√°rio de arquivo\n",
        "        data['doc'] = nlp(data['full_text'])\n",
        "        data['df']['lemmatized_text'] = [\" \".join([token.lemma_.lower() for token in nlp(sent)]) for sent in data['df']['text']]\n",
        "    print(\"‚úÖ Dados para an√°lise individual est√£o prontos.\")\n",
        "else:\n",
        "    print(\"‚ùå Nenhum dado de transcri√ß√£o carregado. Execute a C√©lula 3A ou 3B primeiro.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "SeyIKuD20BGs"
      },
      "outputs": [],
      "source": [
        "# @title ‚òÅÔ∏è 5. Nuvem de Palavras e An√°lise de Frequ√™ncia\n",
        "# @markdown > A nuvem de palavras √© uma representa√ß√£o visual que destaca os termos mais frequentes no texto, permitindo uma r√°pida identifica√ß√£o dos principais temas.\n",
        "\n",
        "# @markdown ### üîπ Par√¢metros de Visualiza√ß√£o\n",
        "# @markdown Defina o n√∫mero m√°ximo de palavras para a nuvem e marque a caixa para ver o gr√°fico de frequ√™ncia.\n",
        "max_words_na_nuvem = 50 #@param {type:\"slider\", min:20, max:300, step:10}\n",
        "mostrar_grafico_de_frequencia = True #@param {type:\"boolean\"}\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "\n",
        "# --- Fun√ß√µes Auxiliares para Gr√°ficos ---\n",
        "def plot_word_cloud(text, max_words):\n",
        "    stopwords = list(spacy.lang.pt.stop_words.STOP_WORDS)\n",
        "    wordcloud = WordCloud(width=1000, height=500, background_color='white', stopwords=stopwords, collocations=True, max_words=max_words).generate(text)\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# --- FUN√á√ÉO DO GR√ÅFICO DE BARRAS ---\n",
        "def plot_frequency_barchart(text, nlp_model, num_words=25):\n",
        "    stopwords = list(spacy.lang.pt.stop_words.STOP_WORDS)\n",
        "    words = [token.text.lower() for token in nlp_model(text) if token.is_alpha and token.text.lower() not in stopwords]\n",
        "    word_counts = Counter(words).most_common(num_words)\n",
        "\n",
        "    if not word_counts:\n",
        "        print(\"N√£o h√° palavras suficientes para gerar o gr√°fico de frequ√™ncia.\")\n",
        "        return\n",
        "\n",
        "    df_freq = pd.DataFrame(word_counts, columns=['Palavra', 'Frequ√™ncia'])\n",
        "\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    ax = sns.barplot(x='Frequ√™ncia', y='Palavra', data=df_freq, color='skyblue')\n",
        "\n",
        "    plt.title(f'As {num_words} Palavras Mais Frequentes', size=16)\n",
        "    plt.xlabel(\"\")\n",
        "    plt.ylabel(\"Palavra\")\n",
        "\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    sns.despine(left=True, bottom=True)\n",
        "\n",
        "    for p in ax.patches:\n",
        "        width = p.get_width()\n",
        "        ax.text(width + (df_freq['Frequ√™ncia'].max() * 0.01),\n",
        "                p.get_y() + p.get_height() / 2,\n",
        "                f'{int(width)}',\n",
        "                ha='left',\n",
        "                va='center',\n",
        "                color='gray',\n",
        "                fontsize=11)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "if 'juntar_arquivos_para_analise_conjunta' in globals() and juntar_arquivos_para_analise_conjunta:\n",
        "    if 'full_text' in globals():\n",
        "        print(\"\\n--- AN√ÅLISE CONJUNTA ---\")\n",
        "\n",
        "        print(\"\\n--- ‚òÅÔ∏è Nuvem de Palavras ---\")\n",
        "        plot_word_cloud(full_text, max_words_na_nuvem)\n",
        "\n",
        "        if mostrar_grafico_de_frequencia:\n",
        "            print(\"\\n--- üìä Gr√°fico de Frequ√™ncia de Palavras ---\")\n",
        "            plot_frequency_barchart(full_text, nlp)\n",
        "\n",
        "    else:\n",
        "        print(\"‚ùå Nenhum dado de an√°lise conjunta encontrado. Execute a C√©lula 4 primeiro.\")\n",
        "\n",
        "elif 'analises_individuais' in globals() and analises_individuais:\n",
        "    print(\"\\n--- AN√ÅLISE INDIVIDUAL ---\")\n",
        "    for filename, data in analises_individuais.items():\n",
        "        if 'full_text' in data:\n",
        "            print(f\"\\n\" + \"=\"*50 + f\"\\nVisualiza√ß√µes para o arquivo: {filename}\\n\" + \"=\"*50)\n",
        "\n",
        "            print(f\"\\n--- ‚òÅÔ∏è Nuvem de Palavras: {filename} ---\")\n",
        "            plot_word_cloud(data['full_text'], max_words_na_nuvem)\n",
        "\n",
        "            if mostrar_grafico_de_frequencia:\n",
        "                print(f\"\\n--- üìä Gr√°fico de Frequ√™ncia: {filename} ---\")\n",
        "                plot_frequency_barchart(data['full_text'], nlp)\n",
        "\n",
        "else:\n",
        "    print(\"‚ùå Nenhum dado de transcri√ß√£o carregado. Execute as C√©lulas 3B e 4 primeiro.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "lC3Ki_T6GkRX"
      },
      "outputs": [],
      "source": [
        "# @title üè∑Ô∏è 6. Reconhecimento de Entidades Nomeadas (NER)\n",
        "# @markdown > O Reconhecimento de Entidades Nomeadas (NER) identifica e classifica nomes de **Pessoas (PER)**, **Locais (LOC)** e **Organiza√ß√µes (ORG)**, sendo √∫til para mapear rapidamente os principais atores e lugares de um discurso.\n",
        "\n",
        "# @markdown >\n",
        "# @markdown > **Importante:** Por ser um processo automatizado, o sistema pode cometer erros, como n√£o identificar uma entidade ou classific√°-la incorretamente. Recomenda-se a revis√£o humana dos resultados para garantir a precis√£o.\n",
        "# @markdown >\n",
        "\n",
        "# @markdown > No modo de an√°lise individual, uma se√ß√£o ao final aponta quais entidades foram mencionadas em mais de um documento.\n",
        "\n",
        "from collections import Counter, defaultdict\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "# Verifica se a an√°lise conjunta foi selecionada\n",
        "if 'juntar_arquivos_para_analise_conjunta' in globals() and juntar_arquivos_para_analise_conjunta:\n",
        "    if 'doc' in globals():\n",
        "        print(\"\\n--- üè∑Ô∏è Reconhecimento de Entidades (An√°lise Conjunta) ---\")\n",
        "        allowed_labels = {'PER', 'LOC', 'ORG'}\n",
        "        entities = [(ent.text, ent.label_) for ent in doc.ents if ent.label_ in allowed_labels]\n",
        "        if not entities:\n",
        "            print(\"Nenhuma entidade dos tipos PER, LOC ou ORG foi encontrada.\")\n",
        "        else:\n",
        "            entity_counts = Counter([ent[0] for ent in entities])\n",
        "            sorted_entities = sorted(entity_counts.items(), key=lambda item: item[1], reverse=True)\n",
        "\n",
        "            html_table = \"<table><tr><th>Entidade</th><th>Tipo</th><th>Frequ√™ncia</th></tr>\"\n",
        "            for entity, count in sorted_entities:\n",
        "                ent_type = [e[1] for e in entities if e[0] == entity][0]\n",
        "                html_table += f\"<tr><td>{entity}</td><td>{ent_type}</td><td>{count}</td></tr>\"\n",
        "            html_table += \"</table>\"\n",
        "            display(HTML(html_table))\n",
        "    else:\n",
        "        print(\"‚ùå Nenhum dado de an√°lise conjunta encontrado. Execute as c√©lulas anteriores primeiro.\")\n",
        "\n",
        "# Verifica se a an√°lise individual foi selecionada\n",
        "elif 'analises_individuais' in globals() and analises_individuais:\n",
        "    print(\"\\n--- AN√ÅLISE INDIVIDUAL ---\")\n",
        "    all_entity_counts = {}\n",
        "    entity_to_type = {}\n",
        "\n",
        "    for filename, data in analises_individuais.items():\n",
        "        if 'doc' in data:\n",
        "            print(f\"\\n--- üè∑Ô∏è Reconhecimento de Entidades para: {filename} ---\")\n",
        "            allowed_labels = {'PER', 'LOC', 'ORG'}\n",
        "            entities = [(ent.text, ent.label_) for ent in data['doc'].ents if ent.label_ in allowed_labels]\n",
        "\n",
        "            if not entities:\n",
        "                print(\"Nenhuma entidade dos tipos PER, LOC ou ORG foi encontrada neste arquivo.\")\n",
        "            else:\n",
        "                counts = Counter([ent[0] for ent in entities])\n",
        "                all_entity_counts[filename] = counts\n",
        "                for entity, label in entities:\n",
        "                    if entity not in entity_to_type:\n",
        "                        entity_to_type[entity] = label\n",
        "\n",
        "                sorted_entities = sorted(counts.items(), key=lambda item: item[1], reverse=True)\n",
        "                html_table = \"<table><tr><th>Entidade</th><th>Tipo</th><th>Frequ√™ncia</th></tr>\"\n",
        "                for entity, count in sorted_entities:\n",
        "                    html_table += f\"<tr><td>{entity}</td><td>{entity_to_type[entity]}</td><td>{count}</td></tr>\"\n",
        "                html_table += \"</table>\"\n",
        "                display(HTML(html_table))\n",
        "\n",
        "    # --- AN√ÅLISE COMPARATIVA EM TEXTO ---\n",
        "    if len(all_entity_counts) > 1:\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"--- üîé An√°lise de Entidades em Comum entre Documentos ---\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        master_entity_count = Counter()\n",
        "        for filename in all_entity_counts:\n",
        "            master_entity_count.update(all_entity_counts[filename].keys())\n",
        "\n",
        "        common_entities = {entity for entity, count in master_entity_count.items() if count > 1}\n",
        "\n",
        "        if not common_entities:\n",
        "            print(\"\\nNenhuma entidade nomeada foi encontrada em mais de um documento.\")\n",
        "        else:\n",
        "            print(\"\\nAs seguintes entidades foram mencionadas em m√∫ltiplos documentos:\\n\")\n",
        "            for entity in sorted(list(common_entities)):\n",
        "                doc_mentions = []\n",
        "                ent_type = entity_to_type[entity]\n",
        "\n",
        "                for filename, counts in all_entity_counts.items():\n",
        "                    if entity in counts:\n",
        "                        count = counts[entity]\n",
        "                        plural = \"vezes\" if count > 1 else \"vez\"\n",
        "                        doc_mentions.append(f\"no documento '{filename}' ({count} {plural})\")\n",
        "\n",
        "                if len(doc_mentions) == 2:\n",
        "                    mentions_str = \" e \".join(doc_mentions)\n",
        "                else:\n",
        "                    mentions_str = \", \".join(doc_mentions[:-1]) + \", e \" + doc_mentions[-1]\n",
        "\n",
        "                print(f\"üîπ A entidade '{entity}' (tipo: {ent_type}) foi encontrada {mentions_str}.\")\n",
        "else:\n",
        "    print(\"‚ùå Nenhum dado de transcri√ß√£o carregado. Execute a C√©lula 3A ou 3B primeiro.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "w3VYDhzDytd0"
      },
      "outputs": [],
      "source": [
        "# @title üìà 7. Contagem de Frequ√™ncia de Palavra Espec√≠fica\n",
        "# @markdown > Esta c√©lula conta quantas vezes uma **palavra ou frase** aparece nas transcri√ß√µes.\n",
        "# @markdown > Voc√™ pode optar por buscar a forma exata ou considerar **varia√ß√µes lingu√≠sticas** da palavra ao retornar ela a sua raiz usando lematiza√ß√£o.\n",
        "\n",
        "# @markdown ### üîπ Defina os par√¢metros da sua busca\n",
        "# @markdown Digite a palavra ou termo que deseja contar nos textos.\n",
        "\n",
        "# @markdown **Palavra ou Frase para Contar:**\n",
        "termo_alvo = \"rio\" # @param {type:\"string\"}\n",
        "\n",
        "# @markdown ---\n",
        "# @markdown **Op√ß√µes de An√°lise:**\n",
        "# @markdown Marque a caixa para contar todas as varia√ß√µes da palavra (ex: buscar por \"correr\" tamb√©m contar√° \"correu\", \"correndo\", etc.).\n",
        "usar_lematizacao = True #@param {type:\"boolean\"}\n",
        "\n",
        "\n",
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "def contar_frequencia(doc, term, use_lemmas, nlp_model):\n",
        "    target_term = term.strip().lower()\n",
        "    if not target_term:\n",
        "        print(\"O termo de busca est√° vazio.\")\n",
        "        return\n",
        "\n",
        "    if use_lemmas:\n",
        "        target_form = \" \".join([token.lemma_.lower() for token in nlp_model(target_term)])\n",
        "        target_display = f\"{term.strip()} (raiz: {target_form})\"\n",
        "        text_to_search = \" \".join([token.lemma_.lower() for token in doc])\n",
        "    else:\n",
        "        target_form = target_term\n",
        "        target_display = target_term\n",
        "        text_to_search = doc.text.lower()\n",
        "\n",
        "    count = len(re.findall(r'\\b' + re.escape(target_form) + r'\\b', text_to_search))\n",
        "\n",
        "    if count > 0:\n",
        "        plural = \"vezes\" if count > 1 else \"vez\"\n",
        "        print(f\"‚úÖ O termo '{target_display}' foi encontrado {count} {plural}.\")\n",
        "    else:\n",
        "        print(f\"‚ùå O termo '{target_display}' n√£o foi encontrado.\")\n",
        "\n",
        "\n",
        "if 'juntar_arquivos_para_analise_conjunta' in globals() and juntar_arquivos_para_analise_conjunta:\n",
        "    if 'doc' in globals():\n",
        "        print(\"--- üîé Contagem de Frequ√™ncia para a AN√ÅLISE CONJUNTA ---\")\n",
        "        contar_frequencia(doc, termo_alvo, usar_lematizacao, nlp)\n",
        "    else:\n",
        "        print(\"‚ùå Nenhum dado de an√°lise conjunta encontrado. Execute as C√©lulas 3 e 4 primeiro.\")\n",
        "\n",
        "elif 'analises_individuais' in globals() and analises_individuais:\n",
        "    print(\"--- üîé Contagem de Frequ√™ncia para a AN√ÅLISE INDIVIDUAL ---\")\n",
        "    for filename, data in analises_individuais.items():\n",
        "        if 'doc' in data:\n",
        "            print(f\"\\n\\n--- üìú Resultados para o arquivo: {filename} ---\")\n",
        "            contar_frequencia(data['doc'], termo_alvo, usar_lematizacao, nlp)\n",
        "        else:\n",
        "            print(f\"‚ùå Dados para o arquivo '{filename}' n√£o est√£o prontos. Execute a C√©lula 4 primeiro.\")\n",
        "else:\n",
        "    print(\"‚ùå Nenhum dado de transcri√ß√£o carregado. Execute as C√©lulas 3 e 4 primeiro.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "YAuoD8m_LW_W"
      },
      "outputs": [],
      "source": [
        "# @title üß† 8. Prepara√ß√£o dos Modelos para An√°lise\n",
        "# @markdown > **Execute esta c√©lula antes de prosseguir.**\n",
        "# @markdown >\n",
        "# @markdown > Ela prepara os modelos de IA necess√°rios tanto para a **Busca Sem√¢ntica** (na C√©lula 9) quanto para a **Auto-Codifica√ß√£o Sem√¢ntica** (na C√©lula 12).\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import torch\n",
        "from collections import Counter\n",
        "\n",
        "try:\n",
        "    if 'embedder' not in globals():\n",
        "        print(\"Carregando o modelo de busca sem√¢ntica (SentenceTransformer)...\")\n",
        "        embedder = SentenceTransformer('distiluse-base-multilingual-cased-v1')\n",
        "        print(\"‚úÖ Modelo de busca carregado.\")\n",
        "\n",
        "    # --- An√°lise Conjunta ---\n",
        "    if 'juntar_arquivos_para_analise_conjunta' in globals() and juntar_arquivos_para_analise_conjunta:\n",
        "        if 'df_transcription' in globals() and 'doc' in globals():\n",
        "            print(\"\\nPreparando dados para a AN√ÅLISE CONJUNTA...\")\n",
        "            # 1. Embeddings de Senten√ßas (para busca por frase)\n",
        "            if 'corpus_embeddings' not in globals():\n",
        "                print(\" -> Gerando embeddings de senten√ßas...\")\n",
        "                corpus_embeddings = embedder.encode(df_transcription['text'].tolist(), convert_to_tensor=True)\n",
        "            # 2. Embeddings de Palavras (para a nova busca por palavras similares)\n",
        "            if 'word_embeddings' not in globals():\n",
        "                print(\" -> Gerando embeddings de palavras...\")\n",
        "                words = Counter([token.lemma_.lower() for token in doc if token.is_alpha and not token.is_stop])\n",
        "                unique_words = list(words.keys())\n",
        "                vocab_embeddings = embedder.encode(unique_words, convert_to_tensor=True, show_progress_bar=True)\n",
        "                word_embeddings = {'words': unique_words, 'embeddings': vocab_embeddings}\n",
        "            print(\"‚úÖ Dados para an√°lise conjunta est√£o prontos!\")\n",
        "        else:\n",
        "            print(\"‚ùå Nenhum dado de an√°lise conjunta encontrado. Execute as C√©lulas 3 e 4.\")\n",
        "\n",
        "    # --- An√°lise Individual ---\n",
        "    elif 'analises_individuais' in globals() and analises_individuais:\n",
        "        print(\"\\nPreparando dados para a AN√ÅLISE INDIVIDUAL...\")\n",
        "        for filename, data in analises_individuais.items():\n",
        "            if 'df' in data and 'doc' in data:\n",
        "                print(f\"\\nProcessando '{filename}'...\")\n",
        "                # 1. Embeddings de Senten√ßas\n",
        "                if 'corpus_embeddings' not in data:\n",
        "                    print(\" -> Gerando embeddings de senten√ßas...\")\n",
        "                    data['corpus_embeddings'] = embedder.encode(data['df']['text'].tolist(), convert_to_tensor=True)\n",
        "                # 2. Embeddings de Palavras\n",
        "                if 'word_embeddings' not in data:\n",
        "                    print(\" -> Gerando embeddings de palavras...\")\n",
        "                    words = Counter([token.lemma_.lower() for token in data['doc'] if token.is_alpha and not token.is_stop])\n",
        "                    unique_words = list(words.keys())\n",
        "                    vocab_embeddings = embedder.encode(unique_words, convert_to_tensor=True, show_progress_bar=False)\n",
        "                    data['word_embeddings'] = {'words': unique_words, 'embeddings': vocab_embeddings}\n",
        "        print(\"\\n‚úÖ Dados para an√°lise individual est√£o prontos!\")\n",
        "    else:\n",
        "        print(\"‚ùå Nenhum dado de transcri√ß√£o carregado. Execute a C√©lula 3 e 4 primeiro.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Ocorreu um erro ao preparar os modelos de busca: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "guNmNgLISJRd"
      },
      "outputs": [],
      "source": [
        "# @title üîç 9. Busca Interativa (Booleana, Lema e Sem√¢ntica)\n",
        "# @markdown > Esta c√©lula permite realizar buscas interativas nos textos carregados.\n",
        "# @markdown > Voc√™ pode buscar por palavras exatas, ra√≠zes (lemas), ideias semelhantes ou palavras semanticamente pr√≥ximas, com foco em temas como meio ambiente e impactos nos rios.\n",
        "\n",
        "# @markdown > **Tipos de Busca Dispon√≠veis:**\n",
        "# @markdown >\n",
        "# @markdown > üîπ **Busca Exata (Booleana):**\n",
        "# @markdown > Retorna frases que contenham exatamente os termos digitados.\n",
        "# @markdown > Permite uso de operadores como **AND**, **OR**, par√™nteses `( )` e aspas `\" \"` para frases espec√≠ficas.\n",
        "# @markdown > Exemplo: `(rio AND \"problema ambiental\")`\n",
        "# @markdown >\n",
        "# @markdown > üîπ **Busca por Lema (Booleana):**\n",
        "# @markdown > Aplica a l√≥gica booleana considerando a raiz das palavras (lema), permitindo capturar varia√ß√µes lingu√≠sticas.\n",
        "# @markdown > Exemplo: `(poluir AND peixe)` tamb√©m encontra frases com \"polui√ß√£o\", \"poluindo\", \"peixes\", etc.\n",
        "# @markdown >\n",
        "# @markdown > üîπ **Busca Sem√¢ntica (por contexto):**\n",
        "# @markdown > Compara **frases inteiras** por similaridade de significado, √∫til para encontrar ideias pr√≥ximas mesmo com vocabul√°rio diferente.\n",
        "# @markdown > Use **OR** para buscar por m√∫ltiplos conceitos.\n",
        "# @markdown > Exemplo: `desmatamento OR contamina√ß√£o h√≠drica`\n",
        "# @markdown >\n",
        "# @markdown > üîπ **Busca Sem√¢ntica (por palavras similares):**\n",
        "# @markdown > Encontra frases que contenham **sin√¥nimos ou palavras associadas** ao termo-alvo.\n",
        "# @markdown > Exemplo: `agrot√≥xico` pode retornar frases com \"veneno\", \"herbicida\", \"res√≠duo t√≥xico\", etc.\n",
        "\n",
        "# @markdown ---\n",
        "# @markdown ### üîπ Defina os par√¢metros da sua busca\n",
        "\n",
        "# @markdown **Termo de Busca:**\n",
        "# @markdown Digite as palavras, frases ou conceitos que deseja encontrar.\n",
        "# @markdown ‚Üí Ex: `(rio AND lixo) OR agrot√≥xico`\n",
        "query = \"rio\"  # @param {type:\"string\"}\n",
        "\n",
        "# @markdown **Tipo de Busca:**\n",
        "# @markdown Escolha entre as estrat√©gias de busca dispon√≠veis.\n",
        "search_type = \"Busca por Lema (Booleana)\"  # @param [\"Busca Exata (Booleana)\", \"Busca por Lema (Booleana)\", \"Busca Sem√¢ntica (por contexto)\", \"Busca Sem√¢ntica (por palavras similares)\"]\n",
        "\n",
        "# @markdown **N√≠vel de Similaridade (para buscas sem√¢nticas por contexto):**\n",
        "# @markdown Define o qu√£o parecida a frase precisa ser com seu termo para ser considerada relevante.\n",
        "# @markdown ‚Üí Valores maiores tornam a busca mais precisa (e restrita).\n",
        "nivel_de_similaridade = 0.2  # @param {type:\"slider\", min:0.1, max:1.0, step:0.1}\n",
        "\n",
        "# @markdown **Quantidade de Palavras Similares (para busca por palavras similares):**\n",
        "# @markdown N√∫mero de palavras semanticamente pr√≥ximas que ser√£o consideradas.\n",
        "# @markdown ‚Üí Ex: `rio` pode buscar tamb√©m por \"c√≥rrego\", \"nascente\", \"√°guas\".\n",
        "numero_de_palavras_similares = 10  # @param {type:\"number\"}\n",
        "\n",
        "# @markdown ---\n",
        "# @markdown ### üîπ Par√¢metro de Proximidade (Apenas para Buscas com `AND`)\n",
        "# @markdown Ajusta o tamanho do contexto de busca, definindo a dist√¢ncia m√°xima entre palavras ligadas por **AND**, mesmo se estiverem em frases diferentes. .\n",
        "# @markdown ‚Üí Deixe **0** para exigir que estejam na **mesma frase**.\n",
        "distancia_maxima_palavras = 100  # @param {type:\"number\"}\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "from IPython.display import display, HTML\n",
        "import torch\n",
        "import re\n",
        "from sentence_transformers import util\n",
        "\n",
        "def executar_busca(\n",
        "    dataframe_alvo, query_str, search_type_str, nlp_model,\n",
        "    distancia_max=0, embedder_model=None, corpus_embeddings_alvo=None,\n",
        "    similarity_thr=0.5, numero_similares=10\n",
        "):\n",
        "    query = query_str.strip()\n",
        "    if not query:\n",
        "        print(\"‚ÑπÔ∏è Termo de Busca est√° vazio.\")\n",
        "        return\n",
        "\n",
        "    def format_timestamp(sec):\n",
        "        try:\n",
        "            ms = round((sec - int(sec)) * 1000)\n",
        "            td = pd.to_timedelta(sec, unit='s')\n",
        "            h, m, s = td.components.hours + td.components.days*24, td.components.minutes, td.components.seconds\n",
        "            return f\"{h:02d}:{m:02d}:{s:02d}.{ms:03d}\"\n",
        "        except:\n",
        "            return \"00:00:00.000\"\n",
        "\n",
        "    def run_boolean_search(expr, df, lemma=False):\n",
        "        tokens = re.split(r'(\\sAND\\s|\\sOR\\s|\\(|\\)|\"[^\"]+\")', expr, flags=re.IGNORECASE)\n",
        "        tokens = [t.strip() for t in tokens if t and t.strip()]\n",
        "        pd_tokens, keys = [], []\n",
        "        for t in tokens:\n",
        "            up = t.upper()\n",
        "            if up == 'AND':\n",
        "                pd_tokens.append('&')\n",
        "            elif up == 'OR':\n",
        "                pd_tokens.append('|')\n",
        "            elif t in ('(', ')'):\n",
        "                pd_tokens.append(t)\n",
        "            else:\n",
        "                kw = t.strip('\"'); keys.append(kw)\n",
        "                if lemma:\n",
        "                    lem = \" \".join([w.lemma_ for w in nlp_model(kw.lower())])\n",
        "                    col, term = 'lemmatized_text', re.escape(lem)\n",
        "                else:\n",
        "                    col, term = 'text', re.escape(kw)\n",
        "                pd_tokens.append(f\"df['{col}'].str.contains(r'\\\\b{term}\\\\b', case=False, regex=True)\")\n",
        "        mask = eval(\" \".join(pd_tokens))\n",
        "        return df[mask], keys\n",
        "\n",
        "    html = \"<table>\"\n",
        "    found = False\n",
        "    bool_search = search_type_str.startswith('Busca Exata') or search_type_str.startswith('Busca por Lema')\n",
        "    lemma_search = (search_type_str == 'Busca por Lema (Booleana)')\n",
        "\n",
        "    # === BOOLEANA / LEMA ===\n",
        "    if bool_search:\n",
        "        clauses = [c.strip() for c in re.split(r'\\s+OR\\s+', query, flags=re.IGNORECASE)]\n",
        "        combined_indices = set()\n",
        "        for clause in clauses:\n",
        "            if 'AND' in clause.upper() and distancia_max > 0:\n",
        "                clean = re.sub(r'[\\(\\)]', '', clause)\n",
        "                terms = [w.strip().strip('\"') for w in re.split(r'\\sAND\\s', clean, flags=re.IGNORECASE)]\n",
        "                if lemma_search:\n",
        "                    search_terms = [\" \".join([w.lemma_ for w in nlp_model(t.lower())]) for t in terms]\n",
        "                    col = 'lemmatized_text'\n",
        "                else:\n",
        "                    search_terms = terms; col = 'text'\n",
        "                for i in range(len(dataframe_alvo)):\n",
        "                    if i in combined_indices: continue\n",
        "                    if re.search(r'\\b'+re.escape(search_terms[0])+r'\\b', dataframe_alvo.iloc[i][col], re.IGNORECASE):\n",
        "                        window, idxs = [], []\n",
        "                        for j in range(i, len(dataframe_alvo)):\n",
        "                            window.append(dataframe_alvo.iloc[j]['text']); idxs.append(j)\n",
        "                            txt = \" \".join(window)\n",
        "                            if len(txt.split()) > distancia_max + 20: break\n",
        "                            check = (\" \".join([w.lemma_.lower() for w in nlp_model(txt)])\n",
        "                                     if lemma_search else txt)\n",
        "                            if all(re.search(r'\\b'+re.escape(t)+r'\\b', check, re.IGNORECASE)\n",
        "                                   for t in search_terms):\n",
        "                                sf = format_timestamp(dataframe_alvo.iloc[idxs[0]]['start'])\n",
        "                                ef = format_timestamp(dataframe_alvo.iloc[idxs[-1]]['end'])\n",
        "                                htxt = txt\n",
        "                                for t in terms:\n",
        "                                    htxt = re.sub(f'({re.escape(t)})',\n",
        "                                                  lambda m: f\"<mark>{m.group(1)}</mark>\",\n",
        "                                                  htxt, flags=re.IGNORECASE)\n",
        "                                html += f\"<tr><td style='white-space:nowrap;'><b>[{sf} ‚Üí {ef}]</b></td><td>{htxt}</td></tr>\"\n",
        "                                found = True\n",
        "                                combined_indices.update(idxs)\n",
        "                                break\n",
        "            else:\n",
        "                df_res, keys = run_boolean_search(clause, dataframe_alvo, lemma_search)\n",
        "                if not df_res.empty:\n",
        "                    for _, row in df_res.iterrows():\n",
        "                        sf = format_timestamp(row['start']); ef = format_timestamp(row['end'])\n",
        "                        txt = row['text']\n",
        "                        for k in keys:\n",
        "                            txt = re.sub(f'({re.escape(k)})',\n",
        "                                         lambda m: f\"<mark>{m.group(1)}</mark>\",\n",
        "                                         txt, flags=re.IGNORECASE)\n",
        "                        html += f\"<tr><td style='white-space:nowrap;'><b>[{sf} ‚Üí {ef}]</b></td><td>{txt}</td></tr>\"\n",
        "                        found = True\n",
        "\n",
        "    # === SEM√ÇNTICA (por contexto) ===\n",
        "    elif search_type_str == 'Busca Sem√¢ntica (por contexto)':\n",
        "        print(f\"Sem√¢ntica: '{query}' (thr={similarity_thr})\")\n",
        "        clauses = [c.strip() for c in re.split(r'\\s+OR\\s+', query, flags=re.IGNORECASE)]\n",
        "        scores = {}\n",
        "        for c in clauses:\n",
        "            df_ex = dataframe_alvo[dataframe_alvo['text'].str.contains(re.escape(c), case=False, regex=True)]\n",
        "            for idx, row in df_ex.iterrows():\n",
        "                scores[idx] = (1.0, c)\n",
        "            emb = embedder_model.encode(c, convert_to_tensor=True)\n",
        "            sims = util.cos_sim(emb, corpus_embeddings_alvo)[0]\n",
        "            for tidx in torch.where(sims >= similarity_thr)[0].tolist():\n",
        "                i, sc = int(tidx), float(sims[tidx])\n",
        "                if i not in scores or sc > scores[i][0]:\n",
        "                    scores[i] = (sc, c)\n",
        "        for i, (sc, cq) in sorted(scores.items(), key=lambda x: -x[1][0]):\n",
        "            row = dataframe_alvo.iloc[i]\n",
        "            sf = format_timestamp(row['start']); ef = format_timestamp(row['end'])\n",
        "            if sc == 1.0:\n",
        "                txt = re.sub(f'({re.escape(cq)})',\n",
        "                             lambda m: f\"<mark>{m.group(1)}</mark>\",\n",
        "                             row['text'], flags=re.IGNORECASE)\n",
        "                tag = \"\"\n",
        "            else:\n",
        "                txt = row['text']; tag = f\" (sim={sc:.2f})\"\n",
        "            html += f\"<tr><td style='white-space:nowrap;'><b>[{sf} ‚Üí {ef}]</b>{tag}</td><td>{txt}</td></tr>\"\n",
        "            found = True\n",
        "\n",
        "    # === SEM√ÇNTICA (por palavras similares) ===\n",
        "    elif search_type_str == 'Busca Sem√¢ntica (por palavras similares)':\n",
        "        if 'word_embeddings' not in globals():\n",
        "            print(\"‚ùå N√£o achei 'word_embeddings' em globals(). Execute a c√©lula de prepara√ß√£o de embeddings de palavras.\")\n",
        "        else:\n",
        "            we = globals()['word_embeddings']\n",
        "            emb = embedder_model.encode(query, convert_to_tensor=True)\n",
        "            sims = util.cos_sim(emb, we['embeddings'])[0]\n",
        "            topk = torch.topk(sims, k=min(numero_similares, len(we['words'])))\n",
        "            similar_words = [we['words'][i] for _, i in zip(topk[0], topk[1])]\n",
        "            print(f\"Palavras similares a '{query}': {', '.join(similar_words)}\")\n",
        "            pattern = r'\\b(' + '|'.join(re.escape(w) for w in similar_words) + r')\\b'\n",
        "            df_res = dataframe_alvo[dataframe_alvo['text'].str.contains(pattern, case=False, regex=True)]\n",
        "            if df_res.empty:\n",
        "                print(\"Nenhum resultado encontrado.\")\n",
        "            else:\n",
        "                for _, row in df_res.iterrows():\n",
        "                    sf = format_timestamp(row['start']); ef = format_timestamp(row['end'])\n",
        "                    txt = re.sub(pattern, lambda m: f\"<mark>{m.group(1)}</mark>\", row['text'], flags=re.IGNORECASE)\n",
        "                    html += f\"<tr><td style='white-space:nowrap;'><b>[{sf} ‚Üí {ef}]</b></td><td>{txt}</td></tr>\"\n",
        "                found = True\n",
        "\n",
        "    if not found:\n",
        "        print(\"Nenhum resultado encontrado.\")\n",
        "    html += \"</table>\"\n",
        "    display(HTML(html))\n",
        "\n",
        "# --- Execu√ß√£o da Busca ---\n",
        "if 'juntar_arquivos_para_analise_conjunta' in globals() and juntar_arquivos_para_analise_conjunta:\n",
        "    if 'df_transcription' in globals() and 'corpus_embeddings' in globals():\n",
        "        print(\"--- An√°lise Conjunta ---\")\n",
        "        executar_busca(\n",
        "            df_transcription, query, search_type, nlp,\n",
        "            distancia_maxima_palavras, embedder, corpus_embeddings, similarity_threshold,\n",
        "            numero_de_palavras_similares\n",
        "        )\n",
        "    else:\n",
        "        print(\"‚ùå Execute C√©lulas 3A ou 3B, 4 e 8 antes.\")\n",
        "elif 'analises_individuais' in globals():\n",
        "    print(\"--- An√°lise Individual ---\")\n",
        "    for fn, data in analises_individuais.items():\n",
        "        print(f\"\\n--- {fn} ---\")\n",
        "        if 'df' in data and 'corpus_embeddings' in data:\n",
        "            if 'word_embeddings' in data:\n",
        "                globals()['word_embeddings'] = data['word_embeddings']\n",
        "            executar_busca(\n",
        "                data['df'], query, search_type, nlp,\n",
        "                distancia_maxima_palavras, embedder,\n",
        "                data['corpus_embeddings'], similarity_threshold,\n",
        "                numero_de_palavras_similares\n",
        "            )\n",
        "        else:\n",
        "            print(f\"‚ùå Execute c√©lulas 4 e 8 para '{fn}'.\")\n",
        "else:\n",
        "    print(\"‚ùå Defina primeiro o modo de an√°lise (C√©lula 3B).\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "P1rCJiOcmHGN"
      },
      "outputs": [],
      "source": [
        "# @title üï∏Ô∏è 10. Grafo de Coocorr√™ncia e Tabela de Frequ√™ncias\n",
        "# @markdown > O Grafo de Coocorr√™ncia √© um \"mapa de ideias\" que mostra quais palavras importantes aparecem frequentemente juntas no mesmo contexto. Sua utilidade √© revelar as principais associa√ß√µes de termos e os grupos tem√°ticos do discurso.\n",
        "\n",
        "# @markdown ### üîπ Par√¢metros do Grafo\n",
        "# @markdown Defina os par√¢metros para a gera√ß√£o do grafo.\n",
        "top_n_palavras = 25 #@param {type:\"slider\", min:10, max:50, step:5}\n",
        "usar_lemas_no_grafo = True #@param {type:\"boolean\"}\n",
        "# @markdown ---\n",
        "# @markdown ### üîπ Defini√ß√£o de Contexto\n",
        "# @markdown Defina o tamanho (em palavras) do contexto a ser analisado para encontrar coocorr√™ncias. **Deixe em 0 para usar as frases originais.**\n",
        "tamanho_do_contexto = 0 #@param {type:\"slider\", min:0, max:100, step:5}\n",
        "# @markdown ---\n",
        "# @markdown ### üîπ Filtro de Relev√¢ncia\n",
        "# @markdown Defina o n√∫mero m√≠nimo de vezes que duas palavras precisam aparecer juntas para criar uma conex√£o.\n",
        "forca_minima_da_conexao = 2 #@param {type:\"slider\", min:1, max:20, step:1}\n",
        "# @markdown ---\n",
        "# @markdown ### üîπ Op√ß√µes de Exibi√ß√£o\n",
        "# @markdown Marque a caixa para exibir tamb√©m a tabela com a frequ√™ncia de cada par.\n",
        "mostrar_tabela_de_frequencia = False #@param {type:\"boolean\"}\n",
        "\n",
        "\n",
        "import networkx as nx\n",
        "from itertools import combinations\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "# --- Fun√ß√£o do Grafo ---\n",
        "def plot_cooccurrence_graph_and_table(doc, top_n=25, use_lemmas=True, chunk_size=0, min_weight=2, show_table=True):\n",
        "\n",
        "    if use_lemmas:\n",
        "        print(\" -> Analisando coocorr√™ncias entre LEMAS (ra√≠zes das palavras).\")\n",
        "        all_tokens = [token.lemma_.lower() for token in doc if token.pos_ in ('NOUN', 'PROPN', 'ADJ', 'ADV') and not token.is_stop and not token.is_punct]\n",
        "    else:\n",
        "        print(\" -> Analisando coocorr√™ncias entre PALAVRAS originais.\")\n",
        "        all_tokens = [token.text.lower() for token in doc if token.pos_ in ('NOUN', 'PROPN', 'ADJ', 'ADV') and not token.is_stop and not token.is_punct]\n",
        "\n",
        "    most_common_words = [word for word, count in Counter(all_tokens).most_common(top_n)]\n",
        "\n",
        "    co_occurrences = []\n",
        "\n",
        "    if chunk_size == 0:\n",
        "        print(\" -> Contexto: Dentro da mesma frase (delimitada pelo Whisper/spaCy).\")\n",
        "        if use_lemmas:\n",
        "            for sentence in doc.sents:\n",
        "                sent_tokens = [token.lemma_.lower() for token in sentence if token.lemma_.lower() in most_common_words]\n",
        "                co_occurrences.extend(list(combinations(sorted(list(set(sent_tokens))), 2)))\n",
        "        else:\n",
        "            for sentence in doc.sents:\n",
        "                sent_tokens = [token.text.lower() for token in sentence if token.text.lower() in most_common_words]\n",
        "                co_occurrences.extend(list(combinations(sorted(list(set(sent_tokens))), 2)))\n",
        "    else:\n",
        "        print(f\" -> Contexto: Trechos de {chunk_size} palavras.\")\n",
        "        filtered_tokens = [token for token in all_tokens if token in most_common_words]\n",
        "\n",
        "        for i in range(0, len(filtered_tokens), chunk_size):\n",
        "            chunk = filtered_tokens[i:i + chunk_size]\n",
        "            co_occurrences.extend(list(combinations(sorted(list(set(chunk))), 2)))\n",
        "\n",
        "    if not co_occurrences:\n",
        "        print(\"\\nN√£o foi poss√≠vel gerar o grafo e a tabela: nenhuma coocorr√™ncia encontrada.\")\n",
        "        return\n",
        "\n",
        "    co_occurrence_counts = Counter(co_occurrences)\n",
        "\n",
        "    # --- Gera√ß√£o do Grafo ---\n",
        "    G = nx.Graph()\n",
        "    for (word1, word2), weight in co_occurrence_counts.items():\n",
        "        if weight >= min_weight:\n",
        "            G.add_edge(word1, word2, weight=weight)\n",
        "\n",
        "    if G.number_of_nodes() == 0:\n",
        "        print(f\"\\nN√£o foi poss√≠vel gerar o grafo: nenhuma coocorr√™ncia encontrada com for√ßa m√≠nima de {min_weight}.\")\n",
        "    else:\n",
        "        plt.figure(figsize=(16, 16))\n",
        "        pos = nx.spring_layout(G, k=0.9, iterations=75, seed=42)\n",
        "        edge_widths = [d['weight'] / 2 for (u, v, d) in G.edges(data=True)]\n",
        "\n",
        "        degrees = [G.degree(n) for n in G.nodes()]\n",
        "        if degrees:\n",
        "            min_degree, max_degree = min(degrees), max(degrees)\n",
        "            min_size, max_size = 500, 5000\n",
        "\n",
        "            if max_degree == min_degree:\n",
        "                node_sizes = [1500] * len(G.nodes())\n",
        "            else:\n",
        "                node_sizes = [\n",
        "                    min_size + (degree - min_degree) * (max_size - min_size) / (max_degree - min_degree)\n",
        "                    for degree in degrees\n",
        "                ]\n",
        "        else:\n",
        "            node_sizes = []\n",
        "\n",
        "        nx.draw_networkx_nodes(G, pos, node_size=node_sizes, node_color='skyblue', alpha=0.9)\n",
        "        nx.draw_networkx_edges(G, pos, width=edge_widths, alpha=0.5, edge_color='gray')\n",
        "        nx.draw_networkx_labels(G, pos, font_size=11, font_family='sans-serif')\n",
        "\n",
        "        plt.title(f'Grafo de Coocorr√™ncia dos {top_n} Termos Mais Comuns', size=18)\n",
        "        plt.box(False)\n",
        "        plt.show()\n",
        "\n",
        "    # --- Gera√ß√£o da Tabela ---\n",
        "    if show_table:\n",
        "        print(\"\\n---  Tabela de Frequ√™ncia de Coocorr√™ncias ---\")\n",
        "        sorted_counts = co_occurrence_counts.most_common()\n",
        "        if not sorted_counts:\n",
        "            print(\"Nenhuma coocorr√™ncia para exibir na tabela.\")\n",
        "            return\n",
        "\n",
        "        df_data = [(item[0][0], item[0][1], item[1]) for item in sorted_counts]\n",
        "        df_cooc = pd.DataFrame(df_data, columns=['Palavra 1', 'Palavra 2', 'Frequ√™ncia'])\n",
        "\n",
        "        pd.set_option('display.max_rows', None)\n",
        "        display(df_cooc)\n",
        "        pd.reset_option('display.max_rows')\n",
        "\n",
        "# --- Execu√ß√£o Principal ---\n",
        "if 'juntar_arquivos_para_analise_conjunta' in globals() and juntar_arquivos_para_analise_conjunta:\n",
        "    if 'doc' in globals():\n",
        "        print(\"\\n--- üï∏Ô∏è Grafo e Tabela de Coocorr√™ncia (An√°lise Conjunta) ---\")\n",
        "        plot_cooccurrence_graph_and_table(doc, top_n=top_n_palavras, use_lemmas=usar_lemas_no_grafo, chunk_size=tamanho_do_contexto, min_weight=forca_minima_da_conexao, show_table=mostrar_tabela_de_frequencia)\n",
        "    else:\n",
        "        print(\"‚ùå Nenhum dado de an√°lise conjunta encontrado. Execute a C√©lula 4 primeiro.\")\n",
        "\n",
        "elif 'analises_individuais' in globals() and analises_individuais:\n",
        "    print(\"\\n--- üï∏Ô∏è Grafo e Tabela de Coocorr√™ncia (An√°lise Individual) ---\")\n",
        "    for filename, data in analises_individuais.items():\n",
        "        if 'doc' in data:\n",
        "            print(f\"\\n--- Resultados para: {filename} ---\")\n",
        "            plot_cooccurrence_graph_and_table(data['doc'], top_n=top_n_palavras, use_lemmas=usar_lemas_no_grafo, chunk_size=tamanho_do_contexto, min_weight=forca_minima_da_conexao, show_table=mostrar_tabela_de_frequencia)\n",
        "else:\n",
        "    print(\"‚ùå Nenhum dado de transcri√ß√£o carregado. Execute as C√©lulas 3B e 4 primeiro.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Fyr2F1xe2-KG"
      },
      "outputs": [],
      "source": [
        "# @title üîé 11. An√°lise de Coloca√ß√£o (Palavras Vizinhas)\n",
        "# @markdown > Esta an√°lise revela quais palavras (vizinhas) aparecem com mais frequ√™ncia antes ou depois de um termo espec√≠fico.\n",
        "\n",
        "# @markdown > Sua utilidade √© entender o contexto de uso de uma palavra e descobrir suas associa√ß√µes e frases mais comuns no discurso.\n",
        "\n",
        "# @markdown ### üîπ Defina os par√¢metros da sua busca\n",
        "# @markdown Digite a palavra que deseja analisar e escolha as op√ß√µes de busca.\n",
        "\n",
        "# @markdown **Palavra-Alvo:**\n",
        "palavra_alvo = \"rio\" # @param {type:\"string\"}\n",
        "\n",
        "# @markdown ---\n",
        "# @markdown **Op√ß√µes de An√°lise:**\n",
        "# @markdown Marque a caixa para agrupar as palavras vizinhas por sua raiz (lema). Ex: \"longa\" e \"longo\" ser√£o contadas juntas como \"longo\".\n",
        "agrupar_palavras_vizinhas_pela_raiz = True #@param {type:\"boolean\"}\n",
        "\n",
        "# @markdown ---\n",
        "# @markdown **Dire√ß√£o da Busca:**\n",
        "# @markdown Escolha se quer ver as palavras que v√™m antes, depois ou em ambas as dire√ß√µes do seu termo.\n",
        "direcao_busca = \"Ambas as dire√ß√µes\" # @param [\"Direita (palavras seguintes)\", \"Esquerda (palavras anteriores)\", \"Ambas as dire√ß√µes\"]\n",
        "\n",
        "# @markdown ---\n",
        "# @markdown **Filtros para Palavras Vizinhas:**\n",
        "# @markdown Selecione as classes gramaticais que voc√™ deseja ver na an√°lise.\n",
        "considerar_substantivos = True #@param {type:\"boolean\"}\n",
        "considerar_adjetivos = True #@param {type:\"boolean\"}\n",
        "considerar_adverbios = True #@param {type:\"boolean\"}\n",
        "considerar_verbos = True #@param {type:\"boolean\"}\n",
        "\n",
        "# @markdown ---\n",
        "# @markdown **Op√ß√µes de Exibi√ß√£o:**\n",
        "mostrar_timestamps = False #@param {type:\"boolean\"}\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "from IPython.display import display, HTML\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "def format_timestamp_display(seconds):\n",
        "    try:\n",
        "        milliseconds = round((seconds - int(seconds)) * 1000)\n",
        "        td = pd.to_timedelta(seconds, unit='s')\n",
        "        h, m, s = td.components.hours + (td.components.days * 24), td.components.minutes, td.components.seconds\n",
        "        return f\"[{h:02d}:{m:02d}:{s:02d}.{milliseconds:03d}]\"\n",
        "    except:\n",
        "        return \"[00:00:00.000]\"\n",
        "\n",
        "def analisar_colocacoes(dataframe_alvo, target_word, group_by_lemma, nlp_model, pos_filters, direction, show_ts):\n",
        "    target_word_processed = target_word.strip().lower()\n",
        "    if not target_word_processed:\n",
        "        print(\"Palavra-alvo est√° vazia.\")\n",
        "        return\n",
        "\n",
        "    allowed_pos = set()\n",
        "    if pos_filters.get('substantivos'): allowed_pos.update(['NOUN', 'PROPN'])\n",
        "    if pos_filters.get('adjetivos'): allowed_pos.add('ADJ')\n",
        "    if pos_filters.get('adverbios'): allowed_pos.add('ADV')\n",
        "    if pos_filters.get('verbos'): allowed_pos.add('VERB')\n",
        "\n",
        "    if not allowed_pos:\n",
        "        print(\"‚ö†Ô∏è Nenhuma classe gramatical foi selecionada para a an√°lise.\")\n",
        "        return\n",
        "\n",
        "    collocations = defaultdict(list)\n",
        "\n",
        "    for index, row in dataframe_alvo.iterrows():\n",
        "        row_doc = nlp_model(row['text'])\n",
        "        for i, token in enumerate(row_doc):\n",
        "            if token.text.lower() == target_word_processed:\n",
        "\n",
        "                # --- L√ìGICA PARA OLHAR PARA A DIREITA ---\n",
        "                if direction in [\"Direita (palavras seguintes)\", \"Ambas as dire√ß√µes\"]:\n",
        "                    for next_token_index in range(i + 1, len(row_doc)):\n",
        "                        next_token = row_doc[next_token_index]\n",
        "                        if next_token.pos_ in allowed_pos and not next_token.is_stop and not next_token.is_punct:\n",
        "                            key = next_token.lemma_.lower() if group_by_lemma else next_token.text.lower()\n",
        "                            collocations[key].append(row['start'])\n",
        "                            break\n",
        "                        elif next_token.is_punct and next_token.text in ['.', '!', '?']:\n",
        "                            break\n",
        "\n",
        "                # --- L√ìGICA PARA OLHAR PARA A ESQUERDA ---\n",
        "                if direction in [\"Esquerda (palavras anteriores)\", \"Ambas as dire√ß√µes\"]:\n",
        "                    for prev_token_index in range(i - 1, -1, -1):\n",
        "                        prev_token = row_doc[prev_token_index]\n",
        "                        if prev_token.pos_ in allowed_pos and not prev_token.is_stop and not prev_token.is_punct:\n",
        "                            key = prev_token.lemma_.lower() if group_by_lemma else prev_token.text.lower()\n",
        "                            collocations[key].append(row['start'])\n",
        "                            break\n",
        "                        elif prev_token.is_punct and prev_token.text in ['.', '!', '?']:\n",
        "                            break\n",
        "\n",
        "    if not collocations:\n",
        "        print(\"Nenhuma coloca√ß√£o encontrada para o termo e filtros especificados.\")\n",
        "        return\n",
        "\n",
        "    sorted_collocations = sorted(collocations.items(), key=lambda item: len(item[1]), reverse=True)\n",
        "\n",
        "    col_name = \"Raiz da Palavra (Lema)\" if group_by_lemma else \"Palavra Vizinha\"\n",
        "    df_data = []\n",
        "\n",
        "    for word, timestamps in sorted_collocations:\n",
        "        freq = len(timestamps)\n",
        "        row_data = {col_name: word, 'Frequ√™ncia': freq}\n",
        "        if show_ts:\n",
        "            timestamps_str = \", \".join([format_timestamp_display(ts) for ts in sorted(list(set(timestamps)))])\n",
        "            row_data['Timestamps'] = timestamps_str\n",
        "        df_data.append(row_data)\n",
        "\n",
        "    columns = [col_name, 'Frequ√™ncia', 'Timestamps'] if show_ts else [col_name, 'Frequ√™ncia']\n",
        "    df_colloc = pd.DataFrame(df_data, columns=columns)\n",
        "\n",
        "    print(f\"Encontradas {len(df_colloc)} palavras vizinhas √∫nicas.\")\n",
        "    pd.set_option('display.max_rows', None); pd.set_option('display.max_colwidth', None)\n",
        "    display(df_colloc)\n",
        "    pd.reset_option('display.max_rows'); pd.reset_option('display.max_colwidth')\n",
        "\n",
        "# --- Bloco de Execu√ß√£o Principal ---\n",
        "pos_filters_selection = {\n",
        "    \"substantivos\": considerar_substantivos,\n",
        "    \"adjetivos\": considerar_adjetivos,\n",
        "    \"adverbios\": considerar_adverbios,\n",
        "    \"verbos\": considerar_verbos\n",
        "}\n",
        "\n",
        "if 'juntar_arquivos_para_analise_conjunta' in globals() and juntar_arquivos_para_analise_conjunta:\n",
        "    if 'df_transcription' in globals():\n",
        "        print(\"--- üîé An√°lise de Coloca√ß√£o para a AN√ÅLISE CONJUNTA ---\")\n",
        "        analisar_colocacoes(df_transcription, palavra_alvo, agrupar_palavras_vizinhas_pela_raiz, nlp, pos_filters_selection, direcao_busca, mostrar_timestamps)\n",
        "    else:\n",
        "        print(\"‚ùå Nenhum dado de an√°lise conjunta encontrado. Execute as C√©lulas 3 e 4 primeiro.\")\n",
        "\n",
        "elif 'analises_individuais' in globals() and analises_individuais:\n",
        "    print(\"--- üîé An√°lise de Coloca√ß√£o para a AN√ÅLISE INDIVIDUAL ---\")\n",
        "    for filename, data in analises_individuais.items():\n",
        "        if 'df' in data:\n",
        "            print(f\"\\n\\n--- üìú Resultados para o arquivo: {filename} ---\")\n",
        "            analisar_colocacoes(data['df'], palavra_alvo, agrupar_palavras_vizinhas_pela_raiz, nlp, pos_filters_selection, direcao_busca, mostrar_timestamps)\n",
        "        else:\n",
        "            print(f\"‚ùå Dados para o arquivo '{filename}' n√£o est√£o prontos. Execute a C√©lula 4 primeiro.\")\n",
        "else:\n",
        "    print(\"‚ùå Nenhum dado de transcri√ß√£o carregado. Execute as C√©lulas 3 e 4 primeiro.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "bFy89Tc9sfd6"
      },
      "outputs": [],
      "source": [
        "# @title üìö 12.1. Defini√ß√£o do Livro de C√≥digos e Auto-Codifica√ß√£o\n",
        "# @markdown > Defina seus c√≥digos e os conceitos para a auto-codifica√ß√£o sem√¢ntica.\n",
        "\n",
        "# @markdown ### üîπ 1. C√≥digos para Codifica√ß√£o Manual\n",
        "# @markdown Liste todos os seus c√≥digos/temas, separados por **ponto (.)**.\n",
        "lista_de_codigos_manuais = \"Exemplo 1. Exemplo 2. Exemplo 3. Exemplo 4\" # @param {type:\"string\"}\n",
        "\n",
        "# @markdown ---\n",
        "# @markdown ### üîπ 2. Dicion√°rio para Auto-Codifica√ß√£o Sem√¢ntica (Opcional)\n",
        "# @markdown > **Formato:** `C√ìDIGO_1: conceito A, conceito B . C√ìDIGO_2: conceito C`\n",
        "# @markdown > (Use **ponto (.)** para separar diferentes c√≥digos e **v√≠rgula (,)** para m√∫ltiplos conceitos dentro de um c√≥digo).\n",
        "\n",
        "# @markdown >Obs: Requer execu√ß√£o da c√©lula 8.\n",
        "dicionario_auto_codificacao = \"\" # @param {type:\"string\"}\n",
        "\n",
        "# @markdown ---\n",
        "# @markdown ### üîπ 3. Par√¢metros da Auto-Codifica√ß√£o\n",
        "# @markdown Defina o qu√£o similar uma frase precisa ser do conceito para ser auto-codificada.\n",
        "# @markdown > **Recomenda√ß√£o:** Comece com um valor em torno de 0.6. Valores mais altos s√£o mais precisos, mas podem encontrar menos resultados.\n",
        "similaridade_auto_codificacao = 0.45 #@param {type:\"slider\", min:0.1, max:1.0, step:0.05}\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "from sentence_transformers import util\n",
        "import torch\n",
        "\n",
        "print(\"Definindo o livro de c√≥digos...\")\n",
        "manual_codes = [code.strip() for code in re.split(r'\\s*\\.\\s*', lista_de_codigos_manuais) if code.strip()]\n",
        "\n",
        "auto_code_dict = {}\n",
        "for entry in re.split(r'\\s*\\.\\s*', dicionario_auto_codificacao.strip()):\n",
        "    if ':' in entry:\n",
        "        code, concepts_str = entry.split(':', 1)\n",
        "        concepts = [concept.strip() for concept in re.split(r'\\s*,\\s*', concepts_str) if concept.strip()]\n",
        "        if code.strip() and concepts:\n",
        "            auto_code_dict[code.strip()] = concepts\n",
        "\n",
        "all_codes = sorted(list(set(manual_codes + list(auto_code_dict.keys()))))\n",
        "print(f\"‚úÖ C√≥digos definidos: {all_codes}\")\n",
        "\n",
        "\n",
        "def auto_codificar_semantica(df, dictionary, embedder_model, sent_embeddings, threshold):\n",
        "    print(\"\\nIniciando auto-codifica√ß√£o sem√¢ntica...\")\n",
        "    df['codigos'] = [[] for _ in range(len(df))]\n",
        "\n",
        "    all_concepts = [concept for concepts in dictionary.values() for concept in concepts]\n",
        "    if not all_concepts:\n",
        "        print(\"Dicion√°rio de auto-codifica√ß√£o est√° vazio. Pulando esta etapa.\")\n",
        "        return df\n",
        "\n",
        "    concept_embeddings = embedder_model.encode(all_concepts, convert_to_tensor=True)\n",
        "    similarity_matrix = util.cos_sim(sent_embeddings, concept_embeddings)\n",
        "\n",
        "    concept_idx = 0\n",
        "    for code, concepts in dictionary.items():\n",
        "        for concept in concepts:\n",
        "            similar_sentences_indices = torch.where(similarity_matrix[:, concept_idx] >= threshold)[0]\n",
        "\n",
        "            for i_tensor in similar_sentences_indices:\n",
        "                i = i_tensor.item()\n",
        "                if code not in df.at[i, 'codigos']:\n",
        "                    df.at[i, 'codigos'].append(code)\n",
        "            concept_idx += 1\n",
        "\n",
        "    print(\"‚úÖ Auto-codifica√ß√£o conclu√≠da.\")\n",
        "    return df\n",
        "\n",
        "def preparar_dataframe_para_codificacao(df_original, corpus_embeds):\n",
        "    df = df_original.copy()\n",
        "\n",
        "    if auto_code_dict:\n",
        "        if 'embedder' in globals() and corpus_embeds is not None:\n",
        "             df = auto_codificar_semantica(df, auto_code_dict, embedder, corpus_embeds, similaridade_auto_codificacao)\n",
        "        else:\n",
        "            print(\"\\n‚ö†Ô∏è AVISO: Auto-codifica√ß√£o n√£o foi executada. √â preciso executar a C√©lula 8 (Prepara√ß√£o dos Modelos de Busca) primeiro.\")\n",
        "    elif 'codigos' not in df.columns or not all(isinstance(x, list) for x in df['codigos']):\n",
        "        df['codigos'] = [[] for _ in range(len(df))]\n",
        "\n",
        "    return df\n",
        "\n",
        "if 'juntar_arquivos_para_analise_conjunta' in globals() and juntar_arquivos_para_analise_conjunta:\n",
        "    if 'df_transcription' in globals() and 'corpus_embeddings' in globals():\n",
        "        df_transcription = preparar_dataframe_para_codificacao(df_transcription, corpus_embeddings)\n",
        "        print(\"\\nPronto para a codifica√ß√£o da AN√ÅLISE CONJUNTA.\")\n",
        "elif 'analises_individuais' in globals() and analises_individuais:\n",
        "    for filename, data in analises_individuais.items():\n",
        "        if 'df' in data and 'corpus_embeddings' in data:\n",
        "            print(f\"\\n--- Preparando '{filename}' ---\")\n",
        "            data['df'] = preparar_dataframe_para_codificacao(data['df'], data['corpus_embeddings'])\n",
        "    print(\"\\nPronto para a codifica√ß√£o da AN√ÅLISE INDIVIDUAL.\")\n",
        "else:\n",
        "    print(\"‚ùå Nenhum dado carregado. Execute as C√©lulas 3, 4 e 8 primeiro.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "lKcDxiUqsl9h"
      },
      "outputs": [],
      "source": [
        "# @title üíª 12.2. Interface de Codifica√ß√£o Manual\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output, Javascript\n",
        "import pandas as pd\n",
        "\n",
        "def format_timestamp(seconds):\n",
        "    try:\n",
        "        milliseconds = round((seconds - int(seconds)) * 1000)\n",
        "        td = pd.to_timedelta(seconds, unit='s')\n",
        "        h, m, s = td.components.hours + (td.components.days * 24), td.components.minutes, td.components.seconds\n",
        "        return f\"{h:02d}:{m:02d}:{s:02d}.{milliseconds:03d}\"\n",
        "    except: return \"00:00:00.000\"\n",
        "\n",
        "target_df = None\n",
        "is_plain_text_mode = False\n",
        "\n",
        "if 'juntar_arquivos_para_analise_conjunta' in globals() and juntar_arquivos_para_analise_conjunta:\n",
        "    if 'df_transcription' in globals():\n",
        "        print(\"Iniciando a codifica√ß√£o para a AN√ÅLISE CONJUNTA.\")\n",
        "        target_df = df_transcription\n",
        "        if len(target_df) > 1 and target_df['start'].iloc[1] == 1.0: is_plain_text_mode = True\n",
        "\n",
        "elif 'analises_individuais' in globals() and analises_individuais:\n",
        "    file_selector = widgets.Dropdown(options=list(analises_individuais.keys()), description='Arquivo:', disabled=False)\n",
        "    print(\"Iniciando a codifica√ß√£o para a AN√ÅLISE INDIVIDUAL.\")\n",
        "    print(\"Selecione o arquivo que deseja codificar:\")\n",
        "    display(file_selector)\n",
        "    target_df = analises_individuais[file_selector.value]['df']\n",
        "    if len(target_df) > 1 and target_df['start'].iloc[1] == 1.0: is_plain_text_mode = True\n",
        "else:\n",
        "    print(\"‚ùå Nenhum dado carregado para codificar.\")\n",
        "\n",
        "if target_df is not None and 'all_codes' in locals() and all_codes:\n",
        "\n",
        "    if 'coding_indices' not in globals():\n",
        "        coding_indices = {filename: 0 for filename in analises_individuais.keys()}\n",
        "        coding_indices['conjunta'] = 0\n",
        "\n",
        "    def get_current_state():\n",
        "        if 'juntar_arquivos_para_analise_conjunta' in globals() and juntar_arquivos_para_analise_conjunta:\n",
        "            key = 'conjunta'; df = df_transcription\n",
        "        else:\n",
        "            key = file_selector.value; df = analises_individuais[key]['df']\n",
        "        return key, df, coding_indices.get(key, 0)\n",
        "\n",
        "    current_key, current_df, current_index = get_current_state()\n",
        "    total_segments = len(current_df)\n",
        "\n",
        "    segment_text_area = widgets.HTML(layout=widgets.Layout(height='auto', border='1px solid lightgray', padding='10px', margin='10px 0'))\n",
        "    checkboxes = {code: widgets.Checkbox(description=code, value=False, indent=False) for code in all_codes}\n",
        "    checkbox_container = widgets.VBox(children=list(checkboxes.values()))\n",
        "    full_text_display = widgets.HTML(layout=widgets.Layout(height='200px', border='1px solid lightgray', padding='10px', overflow='scroll'))\n",
        "    progress_bar = widgets.IntProgress(value=0, min=0, max=total_segments, description='Progresso:', bar_style='info')\n",
        "    prev_button = widgets.Button(description=\"Anterior\", icon='arrow-left')\n",
        "    next_button = widgets.Button(description=\"Pr√≥ximo / Salvar\", icon='arrow-right', button_style='success')\n",
        "    new_code_text = widgets.Text(placeholder='Digite um novo c√≥digo...')\n",
        "    add_code_button = widgets.Button(description=\"Adicionar C√≥digo\")\n",
        "    clear_button = widgets.Button(description=\"Limpar Codifica√ß√µes Deste Arquivo\", icon='trash', button_style='danger')\n",
        "    output_area = widgets.Output()\n",
        "\n",
        "    def update_segment_view(index, df_to_use):\n",
        "        if not (0 <= index < len(df_to_use)): return\n",
        "\n",
        "        progress_bar.value = index + 1\n",
        "        progress_bar.max = len(df_to_use)\n",
        "        segment_row = df_to_use.iloc[index]\n",
        "        saved_codes = segment_row.get('codigos', [])\n",
        "\n",
        "        is_plain = len(df_to_use) > 1 and df_to_use['start'].iloc[1] == 1.0\n",
        "        header = f\"<b>Segmento {index + 1} de {len(df_to_use)}</b>\" if is_plain else f\"<b>[{format_timestamp(segment_row['start'])} --> {format_timestamp(segment_row['end'])}]</b>\"\n",
        "        segment_text_area.value = f\"{header}<br>{segment_row['text']}\"\n",
        "\n",
        "        html_lines = []\n",
        "        for i, text in enumerate(df_to_use['text']):\n",
        "            line_id = f\"segment-scroll-{i}\"\n",
        "            if i == index:\n",
        "                html_lines.append(f\"<span id='{line_id}'><mark>{text}</mark></span>\")\n",
        "            else:\n",
        "                html_lines.append(f\"<span id='{line_id}'>{text}</span>\")\n",
        "\n",
        "        scroll_script = f\"\"\"\n",
        "        <script>\n",
        "            setTimeout(function() {{\n",
        "                var element = document.getElementById('segment-scroll-{index}');\n",
        "                if (element) {{\n",
        "                    element.scrollIntoView({{ behavior: 'smooth', block: 'center', inline: 'nearest' }});\n",
        "                }}\n",
        "            }}, 150);\n",
        "        </script>\n",
        "        \"\"\"\n",
        "        full_text_display.value = \"<br>\".join(html_lines)\n",
        "\n",
        "        for code, cb in checkboxes.items():\n",
        "            cb.value = code in saved_codes\n",
        "\n",
        "    def save_codes_and_go_to(offset):\n",
        "        key, df, current_idx = get_current_state()\n",
        "        selected_codes = [cb.description for cb in checkboxes.values() if cb.value]\n",
        "        df.at[current_idx, 'codigos'] = selected_codes\n",
        "        new_index = current_idx + offset\n",
        "        if 0 <= new_index < len(df):\n",
        "            coding_indices[key] = new_index\n",
        "            update_segment_view(new_index, df)\n",
        "        elif new_index >= len(df):\n",
        "            with output_area: clear_output(); print(\"‚úÖ Codifica√ß√£o de todos os segmentos conclu√≠da para este arquivo!\")\n",
        "\n",
        "    def on_add_code_button_clicked(b):\n",
        "        new_code = new_code_text.value.strip()\n",
        "        if new_code and new_code not in checkboxes:\n",
        "            checkboxes[new_code] = widgets.Checkbox(description=new_code, value=True, indent=False)\n",
        "            checkbox_container.children = list(checkboxes.values())\n",
        "            new_code_text.value = \"\"\n",
        "\n",
        "    def on_clear_button_clicked(b):\n",
        "        with output_area:\n",
        "            clear_output()\n",
        "            key, df, current_idx = get_current_state()\n",
        "            print(f\"Limpando todas as {len(df['codigos'])} codifica√ß√µes do arquivo '{key}'...\")\n",
        "            df['codigos'] = [[] for _ in range(len(df))]\n",
        "            update_segment_view(current_idx, df)\n",
        "            print(\"‚úÖ Todas as codifica√ß√µes foram removidas.\")\n",
        "\n",
        "    def on_file_change(change):\n",
        "        key, df, index = get_current_state()\n",
        "        update_segment_view(coding_indices.get(change.new, 0), analises_individuais[change.new]['df'])\n",
        "\n",
        "    next_button.on_click(lambda b: save_codes_and_go_to(1))\n",
        "    prev_button.on_click(lambda b: save_codes_and_go_to(-1))\n",
        "    add_code_button.on_click(on_add_code_button_clicked)\n",
        "    clear_button.on_click(on_clear_button_clicked)\n",
        "\n",
        "    if 'file_selector' in locals():\n",
        "      file_selector.observe(on_file_change, names='value')\n",
        "\n",
        "    update_segment_view(current_index, current_df)\n",
        "\n",
        "    ui = widgets.VBox([\n",
        "        progress_bar, segment_text_area,\n",
        "        widgets.Label(\"Selecione os c√≥digos aplic√°veis:\"), checkbox_container,\n",
        "        widgets.HBox([widgets.Label(\"Adicionar novo c√≥digo:\"), new_code_text, add_code_button]),\n",
        "        widgets.HBox([prev_button, next_button, clear_button]),\n",
        "        output_area,\n",
        "        widgets.Label(\"Contexto do Texto Completo (segmento atual em destaque):\"), full_text_display\n",
        "    ])\n",
        "    display(ui)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "b_2wluuRsouo"
      },
      "outputs": [],
      "source": [
        "# @title üìä 12.3. An√°lise dos Resultados da Codifica√ß√£o\n",
        "# @markdown > Esta c√©lula analisa os c√≥digos que voc√™ aplicou, mostrando a frequ√™ncia de cada tema e permitindo que voc√™ recupere todas as frases associadas a um c√≥digo espec√≠fico.\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "def analisar_codigos(df, title):\n",
        "    print(\"\\n\" + \"=\"*50 + f\"\\nAnalisando Resultados para: {title}\\n\" + \"=\"*50)\n",
        "\n",
        "    if 'codigos' not in df.columns or not all(isinstance(x, list) for x in df['codigos']):\n",
        "        print(\"Coluna 'codigos' n√£o encontrada ou em formato inv√°lido. Execute a C√©lula 12.1 e 12.2 primeiro.\")\n",
        "        return\n",
        "\n",
        "    all_codes_applied = [code for sublist in df['codigos'] for code in sublist]\n",
        "    if not all_codes_applied:\n",
        "        print(\"Nenhum c√≥digo foi aplicado a este documento ainda.\")\n",
        "        return\n",
        "\n",
        "    code_counts = Counter(all_codes_applied)\n",
        "    df_freq = pd.DataFrame(code_counts.most_common(), columns=['C√≥digo', 'Frequ√™ncia'])\n",
        "\n",
        "    print(\"\\n--- üìä Frequ√™ncia dos C√≥digos ---\")\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    ax = sns.barplot(x='Frequ√™ncia', y='C√≥digo', data=df_freq, color='skyblue')\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    sns.despine(left=True, bottom=True)\n",
        "    plt.title('Frequ√™ncia de Cada C√≥digo Aplicado', size=16)\n",
        "    for p in ax.patches:\n",
        "        width = p.get_width()\n",
        "        ax.text(width + (df_freq['Frequ√™ncia'].max() * 0.01), p.get_y() + p.get_height() / 2, f'{int(width)}', ha='left', va='center', color='gray')\n",
        "    plt.show()\n",
        "\n",
        "    print(\"\\n--- üìù Recuperar Segmentos por C√≥digo ---\")\n",
        "    print(\"Selecione um c√≥digo para ver todas as frases associadas a ele.\")\n",
        "\n",
        "    code_selector = widgets.Dropdown(options=sorted(list(code_counts.keys())))\n",
        "    button = widgets.Button(description=\"Mostrar Frases\")\n",
        "    output = widgets.Output()\n",
        "\n",
        "    def on_button_click(b):\n",
        "        with output:\n",
        "            clear_output()\n",
        "            selected_code = code_selector.value\n",
        "            filtered_df = df[df['codigos'].apply(lambda codes: selected_code in codes)].copy()\n",
        "\n",
        "            merged_data = []\n",
        "            if not filtered_df.empty:\n",
        "                current_group = []\n",
        "                for index, row in filtered_df.iterrows():\n",
        "                    if not current_group or index == current_group[-1]['index'] + 1:\n",
        "                        current_group.append({'index': index, 'row': row})\n",
        "                    else:\n",
        "                        merged_data.append(current_group)\n",
        "                        current_group = [{'index': index, 'row': row}]\n",
        "                if current_group:\n",
        "                    merged_data.append(current_group)\n",
        "            # --------------------------------------------------\n",
        "\n",
        "            if not merged_data:\n",
        "                print(f\"Nenhuma frase encontrada para o c√≥digo: '{selected_code}'\")\n",
        "            else:\n",
        "                html_table = \"<table>\"\n",
        "                is_plain = len(df) > 1 and df['start'].iloc[1] == 1.0\n",
        "                for group in merged_data:\n",
        "                    start_row, end_row = group[0]['row'], group[-1]['row']\n",
        "                    if is_plain:\n",
        "                        header = f\"<b>Segmento {group[0]['index'] + 1} a {group[-1]['index'] + 1}</b>\"\n",
        "                    else:\n",
        "                        header = f\"<b>[{format_timestamp(start_row['start'])} --> {format_timestamp(end_row['end'])}]</b>\"\n",
        "\n",
        "                    combined_text = \" \".join([item['row']['text'] for item in group])\n",
        "                    html_table += f\"<tr><td style='white-space:nowrap; vertical-align: top;'>{header}</td><td>{combined_text}</td></tr>\"\n",
        "                html_table += \"</table>\"\n",
        "                display(HTML(html_table))\n",
        "\n",
        "    button.on_click(on_button_click)\n",
        "    display(widgets.HBox([code_selector, button]), output)\n",
        "\n",
        "if 'juntar_arquivos_para_analise_conjunta' in globals() and juntar_arquivos_para_analise_conjunta:\n",
        "    if 'df_transcription' in globals():\n",
        "        analisar_codigos(df_transcription, \"An√°lise Conjunta\")\n",
        "elif 'analises_individuais' in globals() and analises_individuais:\n",
        "    for filename, data in analises_individuais.items():\n",
        "        if 'df' in data:\n",
        "            analisar_codigos(data['df'], filename)\n",
        "else:\n",
        "    print(\"‚ùå Nenhum dado de transcri√ß√£o carregado.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
